[
  {
    "full_name": "tambo-ai/tambo",
    "name": "tambo",
    "description": "Generative UI SDK for React",
    "language": "TypeScript",
    "today_stars": "300",
    "total_stars": "9291",
    "metadata": {
      "topics": [
        "agent",
        "agents",
        "ai",
        "assistant",
        "assistant-chat-bots",
        "generative-ui",
        "js",
        "react",
        "reactjs",
        "ui",
        "ui-components"
      ],
      "license": "MIT",
      "forks": 450,
      "open_issues": 48,
      "created_at": "2024-06-15",
      "updated_at": "2026-02-13",
      "homepage": "https://tambo.co",
      "default_branch": "main",
      "size_kb": 262748,
      "watchers": 24,
      "archived": false
    },
    "readme_content": "<div align=\"center\">\n  <img src=\"assets/octo-white-background-rounded.png\" width=\"150\">\n  <h1>Tambo AI</h1>\n  <h3>Build agents that speak your UI</h3>\n  <p>The open-source generative UI toolkit for React. Connect your components‚ÄîTambo handles streaming, state management, and MCP.</p>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://www.npmjs.com/package/@tambo-ai/react\"><img src=\"https://img.shields.io/npm/v/%40tambo-ai%2Freact?logo=npm\" alt=\"npm version\" /></a>\n  <a href=\"https://github.com/tambo-ai/tambo/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/tambo-ai/tambo\" alt=\"License\" /></a>\n  <a href=\"https://github.com/tambo-ai/tambo/commits/main\"><img src=\"https://img.shields.io/github/last-commit/tambo-ai/tambo\" alt=\"Last Commit\" /></a>\n  <a href=\"https://discord.gg/dJNvPEHth6\"><img src=\"https://img.shields.io/discord/1251581895414911016?color=7289da&label=discord\" alt=\"Discord\"></a>\n  <a href=\"https://github.com/tambo-ai/tambo\"><img src=\"https://img.shields.io/github/stars/tambo-ai/tambo\" alt=\"GitHub stars\" /></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/15734\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://trendshift.io/api/badge/repositories/15734\" alt=\"tambo-ai/tambo | Trendshift\" width=\"250\" height=\"55\" /></a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://tambo.link/yXkF0hQ\">Start For Free</a> ‚Ä¢\n  <a href=\"https://docs.tambo.co\">Docs</a> ‚Ä¢\n  <a href=\"https://discord.gg/dJNvPEHth6\">Discord</a>\n</p>\n\n---\n\n> **Tambo 1.0 is here!** Read the announcement: [Introducing Tambo: Generative UI for React](https://tambo.co/blog/posts/introducing-tambo-generative-ui)\n\n---\n\n## Table of Contents\n\n- [What is Tambo?](#what-is-tambo)\n- [Get Started](#get-started)\n- [How It Works](#how-it-works)\n- [Features](#features)\n- [How Tambo Compares](#how-tambo-compares)\n- [Community](#community)\n- [License](#license)\n\n## What is Tambo?\n\nTambo is a React toolkit for building agents that render UI (also known as generative UI).\n\nRegister your components with Zod schemas. The agent picks the right one and streams the props so users can interact with them. \"Show me sales by region\" renders your `<Chart>`. \"Add a task\" updates your `<TaskBoard>`.\n\n**[Get started in 5 minutes ‚Üí](#get-started)**\n\nhttps://github.com/user-attachments/assets/8381d607-b878-4823-8b24-ecb8053bef23\n\n### What's Included\n\nTambo is a fullstack solution for adding generative UI to your app. You get a React SDK plus a backend that handles conversation state and agent execution.\n\n**1. Agent included** ‚Äî Tambo runs the LLM conversation loop for you. Bring your own API key (OpenAI, Anthropic, Gemini, Mistral, or any OpenAI-compatible provider). Works with agent frameworks like LangChain and Mastra, but they're not required.\n\n**2. Streaming infrastructure** ‚Äî Props stream to your components as the LLM generates them. Cancellation, error recovery, and reconnection are handled for you.\n\n**3. Tambo Cloud or self-host** ‚Äî Cloud is a hosted backend that manages conversation state and agent orchestration. Self-hosted runs the same backend on your infrastructure via Docker.\n\nMost software is built around a one-size-fits-all mental model. We built Tambo to help developers build software that adapts to users.\n\n## Get Started\n\n```bash\nnpm create tambo-app my-tambo-app  # auto-initializes git + tambo setup\ncd my-tambo-app\nnpm run dev\n```\n\n[**Tambo Cloud**](https://tambo.link/yXkF0hQ) is a hosted backend, free to get started with plenty of credits to start building. **Self-hosted** runs on your own infrastructure.\n\nCheck out the [pre-built component library](https://ui.tambo.co) for agent and generative UI primitives:\n\nhttps://github.com/user-attachments/assets/6cbc103b-9cc7-40f5-9746-12e04c976dff\n\nOr fork a template:\n\n| Template                                                                 | Description                                       |\n| ------------------------------------------------------------------------ | ------------------------------------------------- |\n| [AI Chat with Generative UI](https://github.com/tambo-ai/tambo-template) | Chat interface with dynamic component generation  |\n| [AI Analytics Dashboard](https://github.com/tambo-ai/analytics-template) | Analytics dashboard with AI-powered visualization |\n\n## How It Works\n\nTell the AI which components it can use. Zod schemas define the props. These schemas become LLM tool definitions‚Äîthe agent calls them like functions and Tambo renders the result.\n\n### Generative Components\n\nRender once in response to a message. Charts, summaries, data visualizations.\n\nhttps://github.com/user-attachments/assets/3bd340e7-e226-4151-ae40-aab9b3660d8b\n\n```tsx\nconst components: TamboComponent[] = [\n  {\n    name: \"Graph\",\n    description: \"Displays data as charts using Recharts library\",\n    component: Graph,\n    propsSchema: z.object({\n      data: z.array(z.object({ name: z.string(), value: z.number() })),\n      type: z.enum([\"line\", \"bar\", \"pie\"]),\n    }),\n  },\n];\n```\n\n### Interactable Components\n\nPersist and update as users refine requests. Shopping carts, spreadsheets, task boards.\n\nhttps://github.com/user-attachments/assets/12d957cd-97f1-488e-911f-0ff900ef4062\n\n```tsx\nconst InteractableNote = withInteractable(Note, {\n  componentName: \"Note\",\n  description: \"A note supporting title, content, and color modifications\",\n  propsSchema: z.object({\n    title: z.string(),\n    content: z.string(),\n    color: z.enum([\"white\", \"yellow\", \"blue\", \"green\"]).optional(),\n  }),\n});\n```\n\nDocs: [generative components](https://docs.tambo.co/concepts/generative-interfaces/generative-components), [interactable components](https://docs.tambo.co/concepts/generative-interfaces/interactable-components)\n\n### The Provider\n\nWrap your app with `TamboProvider`. You must provide either `userKey` or `userToken` to identify the thread owner.\n\n```tsx\n<TamboProvider\n  apiKey={process.env.NEXT_PUBLIC_TAMBO_API_KEY!}\n  userKey={currentUserId}\n  components={components}\n>\n  <Chat />\n  <InteractableNote id=\"note-1\" title=\"My Note\" content=\"Start writing...\" />\n</TamboProvider>\n```\n\nUse `userKey` for server-side or trusted environments. Use `userToken` (OAuth access token) for client-side apps where the token contains the user identity. See [User Authentication](https://docs.tambo.co/concepts/user-authentication) for details.\n\nDocs: [provider options](https://docs.tambo.co/reference/react-sdk/providers)\n\n### Hooks\n\n`useTambo()` is the primary hook ‚Äî it gives you messages, streaming state, and thread management. `useTamboThreadInput()` handles user input and message submission.\n\n```tsx\nconst { messages, isStreaming } = useTambo();\nconst { value, setValue, submit, isPending } = useTamboThreadInput();\n```\n\nDocs: [threads and messages](https://docs.tambo.co/concepts/conversation-storage), [streaming status](https://docs.tambo.co/concepts/generative-interfaces/component-state), [full tutorial](https://docs.tambo.co/getting-started/quickstart)\n\n## Features\n\n### MCP Integrations\n\nConnect to Linear, Slack, databases, or your own MCP servers. Tambo supports the full MCP protocol: tools, prompts, elicitations, and sampling.\n\n```tsx\nimport { MCPTransport } from \"@tambo-ai/react/mcp\";\n\nconst mcpServers = [\n  {\n    name: \"filesystem\",\n    url: \"http://localhost:8261/mcp\",\n    transport: MCPTransport.HTTP,\n  },\n];\n\n<TamboProvider\n  apiKey={process.env.NEXT_PUBLIC_TAMBO_API_KEY!}\n  userKey={currentUserId}\n  components={components}\n  mcpServers={mcpServers}\n>\n  <App />\n</TamboProvider>;\n```\n\nhttps://github.com/user-attachments/assets/c7a13915-8fed-4758-be1b-30a60fad0cda\n\nDocs: [MCP integration](https://docs.tambo.co/concepts/model-context-protocol)\n\n### Local Tools\n\nSometimes you need functions that run in the browser. DOM manipulation, authenticated fetches, accessing React state. Define them as tools and the AI can call them.\n\n```tsx\nconst tools: TamboTool[] = [\n  {\n    name: \"getWeather\",\n    description: \"Fetches weather for a location\",\n    tool: async (params: { location: string }) =>\n      fetch(`/api/weather?q=${encodeURIComponent(params.location)}`).then((r) =>\n        r.json(),\n      ),\n    inputSchema: z.object({\n      location: z.string(),\n    }),\n    outputSchema: z.object({\n      temperature: z.number(),\n      condition: z.string(),\n      location: z.string(),\n    }),\n  },\n];\n\n<TamboProvider\n  apiKey={process.env.NEXT_PUBLIC_TAMBO_API_KEY!}\n  userKey={currentUserId}\n  tools={tools}\n  components={components}\n>\n  <App />\n</TamboProvider>;\n```\n\nDocs: [local tools](https://docs.tambo.co/guides/take-actions/register-tools)\n\n### Context, Auth, and Suggestions\n\n**Additional context** lets you pass metadata to give the AI better responses. User state, app settings, current page. **User authentication** passes tokens from your auth provider. **Suggestions** generates prompts users can click based on what they're doing.\n\n```tsx\n<TamboProvider\n  apiKey={process.env.NEXT_PUBLIC_TAMBO_API_KEY!}\n  userToken={userToken}\n  contextHelpers={{\n    selectedItems: () => ({\n      key: \"selectedItems\",\n      value: selectedItems.map((i) => i.name).join(\", \"),\n    }),\n    currentPage: () => ({ key: \"page\", value: window.location.pathname }),\n  }}\n/>\n```\n\n```tsx\nconst { suggestions, accept } = useTamboSuggestions({ maxSuggestions: 3 });\n\nsuggestions.map((s) => (\n  <button key={s.id} onClick={() => accept(s)}>\n    {s.title}\n  </button>\n));\n```\n\nDocs: [additional context](https://docs.tambo.co/concepts/additional-context), [user authentication](https://docs.tambo.co/concepts/user-authentication), [suggestions](https://docs.tambo.co/concepts/suggestions)\n\n### Supported LLM Providers\n\nOpenAI, Anthropic, Cerebras, Google Gemini, Mistral, and any OpenAI-compatible provider. [Full list](https://docs.tambo.co/reference/llm-providers). Missing one? [Let us know](https://github.com/tambo-ai/tambo/issues).\n\n## How Tambo Compares\n\n| Feature                            | Tambo                                 | Vercel AI SDK                    | CopilotKi",
    "manifest_content": "{\n  \"name\": \"@tambo-ai/repo\",\n  \"version\": \"1.0.0\",\n  \"description\": \"<p align=\\\"center\\\">   <img src=\\\"github-hydra-ai.png\\\" alt=\\\"Hydra AI Logo\\\"> </p> <p align=\\\"center\\\">   <a href=\\\"https://www.npmjs.com/package/hydra-ai\\\"><img src=\\\"https://img.shields.io/npm/v/hydra-ai.svg\\\" alt=\\\"npm version\\\"></a>   <a href=\\\"https://www.npmjs.com/package/hydra-ai\\\"><img src=\\\"https://img.shields.io/npm/dm/hydra-ai.svg\\\" alt=\\\"npm downloads\\\"></a>   <a href=\\\"https://github.com/michaelmagan/hydraai/blob/main/LICENSE\\\"><img src=\\\"https://img.shields.io/github/license/michaelmagan/hydraai.svg\\\" alt=\\\"license\\\"></a>   <a href=\\\"https://github.com/michaelmagan/hydraai/commits/main\\\"><img src=\\\"https://img.shields.io/github/last-commit/michaelmagan/hydraai.svg\\\" alt=\\\"GitHub last commit\\\"></a>   <a href=\\\"https://discord.gg/dJNvPEHth6\\\"><img src=\\\"https://img.shields.io/discord/1251581895414911016?color=7289da&label=discord\\\" alt=\\\"Discord\\\"></a>     <a href=\\\"https://github.com/michaelmagan/hydraai/stargazers\\\"><img src=\\\"https://img.shields.io/github/stars/michaelmagan/hydraai.svg?style=social\\\" alt=\\\"GitHub stars\\\"></a>\",\n  \"scripts\": {\n    \"tambo:setup\": \"bash ./scripts/cloud/tambo-setup.sh\",\n    \"tambo:start\": \"bash ./scripts/cloud/tambo-start.sh\",\n    \"tambo:stop\": \"bash ./scripts/cloud/tambo-stop.sh\",\n    \"build\": \"turbo build\",\n    \"build:sdk\": \"turbo build --filter=@tambo-ai/react\",\n    \"check-types\": \"turbo check-types\",\n    \"clean\": \"turbo clean\",\n    \"format\": \"prettier --write \\\"**/*.{ts,tsx,md,mdx,yml}\\\"\",\n    \"lint:fix\": \"turbo lint -- --fix\",\n    \"lint\": \"turbo lint\",\n    \"format:code\": \"turbo run format:code\",\n    \"prepare\": \"husky\",\n    \"prettier\": \"prettier\",\n    \"prettier-write\": \"prettier --write .\",\n    \"prettier-check\": \"prettier --check .\",\n    \"test\": \"turbo test\",\n    \"dev\": \"turbo dev --filter=@tambo-ai/showcase --filter=@tambo-ai/docs\",\n    \"dev:sdk\": \"turbo dev --filter=@tambo-ai/react --filter=@tambo-ai/showcase\",\n    \"dev:cloud\": \"turbo dev --filter=@tambo-ai-cloud/web --filter=@tambo-ai-cloud/api\",\n    \"dev:cloud:full\": \"turbo dev --filter=@tambo-ai-cloud/web --filter=@tambo-ai-cloud/api --filter=@tambo-ai/showcase --filter=@tambo-ai/docs\",\n    \"dev:local\": \"./scripts/dev-local.sh\",\n    \"dev:web\": \"turbo dev --filter=@tambo-ai-cloud/web\",\n    \"dev:api\": \"turbo dev --filter=@tambo-ai-cloud/api\",\n    \"dev:docs\": \"turbo dev --filter=@tambo-ai/docs\"\n  },\n  \"author\": \"\",\n  \"license\": \"MIT\",\n  \"devDependencies\": {\n    \"@types/node\": \"^22.19.8\",\n    \"chokidar\": \"^5.0.0\",\n    \"husky\": \"^9.1.7\",\n    \"lint-staged\": \"^16.2.7\",\n    \"prettier\": \"^3.8.1\",\n    \"tsx\": \"^4.21.0\",\n    \"turbo\": \"^2.8.3\",\n    \"type-fest\": \"^5.4.3\",\n    \"typescript\": \"^5.9.3\"\n  },\n  \"packageManager\": \"npm@11.7.0+sha512.c22099a6fff8d5b2286c2a09df5352b4858a7c0c716320f58989d60ad8b29ecf2ce6fdfe97ccb41c23ffb1272e1fa079f868487dd6b81d02a2a9e199c095a117\",\n  \"workspaces\": [\n    \"react-sdk\",\n    \"showcase\",\n    \"docs\",\n    \"cli\",\n    \"create-tambo-app\",\n    \"packages/*\",\n    \"apps/*\"\n  ],\n  \"devEngines\": {\n    \"runtime\": {\n      \"name\": \"node\",\n      \"version\": \">=22\",\n      \"onFail\": \"error\"\n    },\n    \"packageManager\": {\n      \"name\": \"npm\",\n      \"version\": \">=11\",\n      \"onFail\": \"error\"\n    }\n  },\n  \"volta\": {\n    \"node\": \"22.22.0\",\n    \"npm\": \"11.7.0\"\n  }\n}\n"
  },
  {
    "full_name": "danielmiessler/Personal_AI_Infrastructure",
    "name": "Personal_AI_Infrastructure",
    "description": "Agentic AI Infrastructure for magnifying HUMAN capabilities.",
    "language": "TypeScript",
    "today_stars": "351",
    "total_stars": "7763",
    "metadata": {
      "topics": [
        "ai",
        "augmentation",
        "humans",
        "productivity"
      ],
      "license": "MIT",
      "forks": 1108,
      "open_issues": 124,
      "created_at": "2025-09-08",
      "updated_at": "2026-02-13",
      "homepage": "",
      "default_branch": "main",
      "size_kb": 277205,
      "watchers": 126,
      "archived": false
    },
    "readme_content": "<div align=\"center\">\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./images/pai-logo-v7.png\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./images/pai-logo-v7.png\">\n  <img alt=\"PAI Logo\" src=\"./images/pai-logo-v7.png\" width=\"300\">\n</picture>\n\n<br/>\n<br/>\n\n# Personal AI Infrastructure\n\n[![Typing SVG](https://readme-typing-svg.demolab.com?font=Fira+Code&weight=500&size=24&pause=1000&color=60A5FA&center=true&vCenter=true&width=600&lines=Everyone+needs+access+to+the+best+AI.;AI+should+magnify+everyone.;Your+personal+AI+stack.)](https://github.com/danielmiessler/PAI)\n\n<br/>\n\n<!-- Social Proof -->\n![Stars](https://img.shields.io/github/stars/danielmiessler/PAI?style=social)\n![Forks](https://img.shields.io/github/forks/danielmiessler/PAI?style=social)\n![Watchers](https://img.shields.io/github/watchers/danielmiessler/PAI?style=social)\n\n<!-- Project Health -->\n![Release](https://img.shields.io/github/v/release/danielmiessler/PAI?style=flat&logo=github&color=8B5CF6)\n![Last Commit](https://img.shields.io/github/last-commit/danielmiessler/PAI?style=flat&logo=git&color=22C55E)\n![Open Issues](https://img.shields.io/github/issues/danielmiessler/PAI?style=flat&logo=github&color=F97316)\n![Open PRs](https://img.shields.io/github/issues-pr/danielmiessler/PAI?style=flat&logo=github&color=EC4899)\n![License](https://img.shields.io/github/license/danielmiessler/PAI?style=flat&color=60A5FA)\n\n<!-- Metrics -->\n![Discussions](https://img.shields.io/github/discussions/danielmiessler/PAI?style=flat&logo=github&label=Discussions&color=EAB308)\n![Commit Activity](https://img.shields.io/github/commit-activity/m/danielmiessler/PAI?style=flat&logo=git&label=Commits%2Fmo&color=F59E0B)\n![Repo Size](https://img.shields.io/github/repo-size/danielmiessler/PAI?style=flat&logo=database&label=Repo%20Size&color=D97706)\n\n<!-- Content -->\n[![Get Started](https://img.shields.io/badge/üöÄ_Get_Started-Install-22C55E?style=flat)](#-installation)\n[![Release v2.5](https://img.shields.io/badge/üì¶_Release-v2.5-8B5CF6?style=flat)](Releases/v2.5/)\n[![Packs](https://img.shields.io/badge/üì¶_Packs-23-8B5CF6?style=flat)](Packs/)\n[![Bundles](https://img.shields.io/badge/üéÅ_Bundles-1-F97316?style=flat)](Bundles/)\n[![Contributors](https://img.shields.io/github/contributors/danielmiessler/PAI?style=flat&logo=githubsponsors&logoColor=white&label=Contributors&color=EC4899)](https://github.com/danielmiessler/PAI/graphs/contributors)\n\n<!-- Tech Stack -->\n[![Built with Claude](https://img.shields.io/badge/Built_with-Claude-D4A574?style=flat&logo=anthropic&logoColor=white)](https://claude.ai)\n[![TypeScript](https://img.shields.io/badge/TypeScript-3178C6?style=flat&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)\n[![Bun](https://img.shields.io/badge/Bun-000000?style=flat&logo=bun&logoColor=white)](https://bun.sh)\n[![UL Community](https://img.shields.io/badge/UL_Community-5865F2?style=flat&logo=discord&logoColor=white)](https://danielmiessler.com/upgrade)\n\n<br/>\n\n**Overview:** [Purpose](#the-purpose-of-this-project) ¬∑ [What is PAI?](#what-is-pai) ¬∑ [New to AI?](#new-to-this-start-here) ¬∑ [Principles](#the-pai-principles) ¬∑ [Primitives](#pai-primitives)\n\n**Get Started:** [Installation](#-installation) ¬∑ [Releases](Releases/) ¬∑ [Packs](#-packs) ¬∑ [Bundles](#-bundles)\n\n**Resources:** [FAQ](#-faq) ¬∑ [Roadmap](#-roadmap) ¬∑ [Community](#-community) ¬∑ [Contributing](#-contributing)\n\n<br/>\n\n[![PAI Overview Video](https://img.youtube.com/vi/Le0DLrn7ta0/maxresdefault.jpg)](https://youtu.be/Le0DLrn7ta0)\n\n**[Watch the full PAI walkthrough](https://youtu.be/Le0DLrn7ta0)** | **[Read: The Real Internet of Things](https://danielmiessler.com/blog/real-internet-of-things)**\n\n---\n\n</div>\n\n> [!IMPORTANT]\n> **PAI v2.5.0 Released** ‚Äî Think Deeper, Execute Faster: Two-Pass Capability Selection, Thinking Tools with Justify-Exclusion, and Parallel-by-Default Execution.\n>\n> **[Release notes ‚Üí](Releases/v2.5/README.md)** | **[GitHub Release ‚Üí](https://github.com/danielmiessler/PAI/releases/tag/v2.5.0)**\n\n<div align=\"center\">\n\n# AI should magnify everyone‚Äînot just the top 1%.\n\n</div>\n\n## The Purpose of This Project\n\n**PAI exists to solve what I believe is the [P0 problem](https://danielmiessler.com/telos) in the world:**\n\n### Only a tiny fraction of humanity's creative potential is activated on Earth.\n\nMost people don't believe they have valuable contributions to make. They think there are \"special\" people‚Äîand they aren't one of them. They've never asked who they are, what they're about, and have never articulated or written it down. This makes them catastrophically vulnerable to AI displacement. Without activation, there is no high-agency.\n\nSo our goal with PAI is to activate people.\n\n**PAI's mission is twofold:**\n\n1. **Activate as many people as possible** ‚Äî Help people identify, articulate, and pursue their own purpose in life through AI-augmented self-discovery\n2. **Make the best AI available in the world accessible to everyone** ‚Äî Ensure this quality of AI infrastructure isn't reserved for just the rich or technical elite.\n\nThat's why this is an open-source project instead of private.\n\n---\n\n## New to This? Start Here\n\nYou've probably used ChatGPT or Claude. Type a question, get an answer. Simple.\n\nYou can think of AI systems as **three levels**:\n\n<p align=\"center\">\n  <img src=\"./images/pai-eli5-diagram.png\" alt=\"The AI Evolution - From chatbots to your personal AI system\" width=\"800\">\n</p>\n\n### Chatbots\n\nChatGPT, Claude, Gemini‚Äîyou ask something, it answers, and then it forgets everything. Next conversation starts fresh. No memory of you, your preferences, or what you talked about yesterday.\n\n**The pattern:** Ask ‚Üí Answer ‚Üí Forget\n\n### Agentic Platforms\n\nTools like Claude Code, Cursor, and Windsurf. The AI can actually *do* things‚Äîwrite code, browse the web, edit files, run commands.\n\n**The pattern:** Ask ‚Üí Use tools ‚Üí Get result\n\nMore capable, but it still doesn't know *you*‚Äîyour goals, your preferences, your history.\n\n### PAI (Personal AI Infrastructure)\n\nNow your DA **learns and improves**:\n- **Captures every signal** ‚Äî Ratings, sentiment, verification outcomes\n- **Learns from mistakes** ‚Äî Failures get analyzed and fixed\n- **Gets better over time** ‚Äî Success patterns get reinforced\n- **Upgrades itself** ‚Äî Skills, workflows, even the core behavior evolves\n\nPlus it knows:\n- **Your goals** ‚Äî What you're working toward\n- **Your preferences** ‚Äî How you like things done\n- **Your history** ‚Äî Past decisions and learnings\n\n**The pattern:** Observe ‚Üí Think ‚Üí Plan ‚Üí Execute ‚Üí Verify ‚Üí **Learn** ‚Üí Improve\n\nThe key difference: **PAI learns from feedback**. Every interaction makes it better at helping *you* specifically.\n\n---\n\n## What is PAI?\n\nPAI is a Personalized AI Platform designed to magnify your capabilities.\n\nIt's designed for humans most of all, but can be used by teams, companies, or Federations of Planets desiring to be better versions of themselves.\n\nThe scale of the entity doesn't matter: It's a system for understanding, articulating, and realizing its principal's goals using a full-featured Agentic AI Platform.\n\n### Who is PAI for?\n\n**Everyone, full stop.** It's the anti-gatekeeping AI project.\n\n- **Small business owners** who aren't technical but want AI to handle invoicing, scheduling, customer follow-ups, and marketing\n- **Companies** who want to understand their data, optimize operations, and make better decisions\n- **Managers** who want to run their teams more effectively‚Äîtracking projects, preparing for reviews, and communicating clearly\n- **Artists and creatives** who want to find local events, galleries, and opportunities to showcase their work\n- **Everyday people** who want to improve their lives‚Äîbetter fitness routines, stronger social connections, personal finance, or just getting organized\n- **Developers** using AI coding assistants who want persistent memory and custom workflows\n- **Power users** who want their AI to know their goals, preferences, and context\n- **Teams** building shared AI infrastructure with consistent capabilities\n- **Experimenters** interested in AI system design and personal AI patterns\n\n### What makes PAI different?\n\nThe first thing people ask is:\n\n> How is this different from Claude Code, or any of the other agentic systems?\n\nMost agentic systems are built around tools with the user being an afterthought. They are also mostly task-based instead of being goal-based using all the context available to them. PAI is the opposite.\n\n**Three core differentiators:**\n\n1. **Goal Orientation** ‚Äî PAI's primary focus is on the human running it and what they're trying to do in the world, not the tech. This is built into how the system executes all tasks.\n\n2. **Pursuit of Optimal Output** ‚Äî The system's outer loop and everything it does is trying to produce the exact right output given the current situation and all the contexts around it.\n\n3. **Continuous Learning** ‚Äî The system constantly captures signals about what was done, what changes were made, what outputs were produced for each request, and then how you liked or disliked the results.\n\n---\n\n## The PAI Principles\n\nThese principles guide how PAI systems are designed and built. **[Full breakdown ‚Üí](https://danielmiessler.com/blog/personal-ai-infrastructure)**\n\n| # | Principle | Summary |\n|---|-----------|---------|\n| 1 | **User Centricity** | PAI is built around you, not tooling. Your goals, preferences, and context come first‚Äîthe infrastructure exists to serve them. |\n| 2 | **The Foundational Algorithm** | The scientific method as a universal problem-solving loop: Observe ‚Üí Think ‚Üí Plan ‚Üí Build ‚Üí Execute ‚Üí Verify ‚Üí Learn. Define the ideal state, iterate until you reach it. |\n| 3 | **Clear Thinking First** | Good prompts come from clear thinking. Clarify the problem before writing the prompt. |\n| 4 | **Scaffolding > Model** | System architecture matters more than which model you use. |\n| 5 | **Deterministic Infrastructure** | AI is probabilistic; your infrastructure shouldn't be. Us"
  },
  {
    "full_name": "google/langextract",
    "name": "langextract",
    "description": "A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.",
    "language": "Python",
    "today_stars": "1122",
    "total_stars": "31738",
    "metadata": {
      "topics": [
        "gemini",
        "gemini-ai",
        "gemini-api",
        "gemini-flash",
        "gemini-pro",
        "information-extration",
        "large-language-models",
        "llm",
        "nlp",
        "python",
        "structured-data"
      ],
      "license": "Apache-2.0",
      "forks": 2118,
      "open_issues": 119,
      "created_at": "2025-07-08",
      "updated_at": "2026-02-13",
      "homepage": "https://pypi.org/project/langextract/",
      "default_branch": "main",
      "size_kb": 10880,
      "watchers": 155,
      "archived": false
    },
    "readme_content": "<p align=\"center\">\n  <a href=\"https://github.com/google/langextract\">\n    <img src=\"https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg\" alt=\"LangExtract Logo\" width=\"128\" />\n  </a>\n</p>\n\n# LangExtract\n\n[![PyPI version](https://img.shields.io/pypi/v/langextract.svg)](https://pypi.org/project/langextract/)\n[![GitHub stars](https://img.shields.io/github/stars/google/langextract.svg?style=social&label=Star)](https://github.com/google/langextract)\n![Tests](https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg)](https://doi.org/10.5281/zenodo.17015089)\n\n## Table of Contents\n\n- [Introduction](#introduction)\n- [Why LangExtract?](#why-langextract)\n- [Quick Start](#quick-start)\n- [Installation](#installation)\n- [API Key Setup for Cloud Models](#api-key-setup-for-cloud-models)\n- [Adding Custom Model Providers](#adding-custom-model-providers)\n- [Using OpenAI Models](#using-openai-models)\n- [Using Local LLMs with Ollama](#using-local-llms-with-ollama)\n- [More Examples](#more-examples)\n  - [*Romeo and Juliet* Full Text Extraction](#romeo-and-juliet-full-text-extraction)\n  - [Medication Extraction](#medication-extraction)\n  - [Radiology Report Structuring: RadExtract](#radiology-report-structuring-radextract)\n- [Community Providers](#community-providers)\n- [Contributing](#contributing)\n- [Testing](#testing)\n- [Disclaimer](#disclaimer)\n\n## Introduction\n\nLangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.\n\n## Why LangExtract?\n\n1.  **Precise Source Grounding:** Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.\n2.  **Reliable Structured Outputs:** Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.\n3.  **Optimized for Long Documents:** Overcomes the \"needle-in-a-haystack\" challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.\n4.  **Interactive Visualization:** Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.\n5.  **Flexible LLM Support:** Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.\n6.  **Adaptable to Any Domain:** Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.\n7.  **Leverages LLM World Knowledge:** Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.\n\n## Quick Start\n\n> **Note:** Using cloud-hosted models like Gemini requires an API key. See the [API Key Setup](#api-key-setup-for-cloud-models) section for instructions on how to get and configure your key.\n\nExtract structured information with just a few lines of code.\n\n### 1. Define Your Extraction Task\n\nFirst, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.\n\n```python\nimport langextract as lx\nimport textwrap\n\n# 1. Define the prompt and extraction rules\nprompt = textwrap.dedent(\"\"\"\\\n    Extract characters, emotions, and relationships in order of appearance.\n    Use exact text for extractions. Do not paraphrase or overlap entities.\n    Provide meaningful attributes for each entity to add context.\"\"\")\n\n# 2. Provide a high-quality example to guide the model\nexamples = [\n    lx.data.ExampleData(\n        text=\"ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.\",\n        extractions=[\n            lx.data.Extraction(\n                extraction_class=\"character\",\n                extraction_text=\"ROMEO\",\n                attributes={\"emotional_state\": \"wonder\"}\n            ),\n            lx.data.Extraction(\n                extraction_class=\"emotion\",\n                extraction_text=\"But soft!\",\n                attributes={\"feeling\": \"gentle awe\"}\n            ),\n            lx.data.Extraction(\n                extraction_class=\"relationship\",\n                extraction_text=\"Juliet is the sun\",\n                attributes={\"type\": \"metaphor\"}\n            ),\n        ]\n    )\n]\n```\n\n> **Note:** Examples drive model behavior. Each `extraction_text` should ideally be verbatim from the example's `text` (no paraphrasing), listed in order of appearance. LangExtract raises `Prompt alignment` warnings by default if examples don't follow this pattern‚Äîresolve these for best results.\n\n### 2. Run the Extraction\n\nProvide your input text and the prompt materials to the `lx.extract` function.\n\n```python\n# The input text to be processed\ninput_text = \"Lady Juliet gazed longingly at the stars, her heart aching for Romeo\"\n\n# Run the extraction\nresult = lx.extract(\n    text_or_documents=input_text,\n    prompt_description=prompt,\n    examples=examples,\n    model_id=\"gemini-2.5-flash\",\n)\n```\n\n> **Model Selection**: `gemini-2.5-flash` is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, `gemini-2.5-pro` may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the [rate-limit documentation](https://ai.google.dev/gemini-api/docs/rate-limits#tier-2) for details.\n>\n> **Model Lifecycle**: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the [official model version documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions) to stay informed about the latest stable and legacy versions.\n\n### 3. Visualize the Results\n\nThe extractions can be saved to a `.jsonl` file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.\n\n```python\n# Save the results to a JSONL file\nlx.io.save_annotated_documents([result], output_name=\"extraction_results.jsonl\", output_dir=\".\")\n\n# Generate the visualization from the file\nhtml_content = lx.visualize(\"extraction_results.jsonl\")\nwith open(\"visualization.html\", \"w\") as f:\n    if hasattr(html_content, 'data'):\n        f.write(html_content.data)  # For Jupyter/Colab\n    else:\n        f.write(html_content)\n```\n\nThis creates an animated and interactive HTML file:\n\n![Romeo and Juliet Basic Visualization ](https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif)\n\n> **Note on LLM Knowledge Utilization:** This example demonstrates extractions that stay close to the text evidence - extracting \"longing\" for Lady Juliet's emotional state and identifying \"yearning\" from \"gazed longingly at the stars.\" The task could be modified to generate attributes that draw more heavily from the LLM's world knowledge (e.g., adding `\"identity\": \"Capulet family daughter\"` or `\"literary_context\": \"tragic heroine\"`). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.\n\n### Scaling to Longer Documents\n\nFor larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:\n\n```python\n# Process Romeo & Juliet directly from Project Gutenberg\nresult = lx.extract(\n    text_or_documents=\"https://www.gutenberg.org/files/1513/1513-0.txt\",\n    prompt_description=prompt,\n    examples=examples,\n    model_id=\"gemini-2.5-flash\",\n    extraction_passes=3,    # Improves recall through multiple passes\n    max_workers=20,         # Parallel processing for speed\n    max_char_buffer=1000    # Smaller contexts for better accuracy\n)\n```\n\nThis approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. **[See the full *Romeo and Juliet* extraction example ‚Üí](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)** for detailed results and performance insights.\n\n### Vertex AI Batch Processing\n\nSave costs on large-scale tasks by enabling Vertex AI Batch API: `language_model_params={\"vertexai\": True, \"batch\": {\"enabled\": True}}`.\n\nSee an example of the Vertex AI Batch API usage in [this example](docs/examples/batch_api_example.md).\n\n## Installation\n\n### From PyPI\n\n```bash\npip install langextract\n```\n\n*Recommended for most users. For isolated environments, consider using a virtual environment:*\n\n```bash\npython -m venv langextract_env\nsource langextract_env/bin/activate  # On Windows: langextract_env\\Scripts\\activate\npip install langextract\n```\n\n### From Source\n\nLangExtract uses modern Python packaging with `pyproject.toml` for dependency management:\n\n*Installing with `-e` puts the package in development mode, allowing you to modify the code without reinstalling.*\n\n\n```bash\ngit clone https://github.com/google/langextract.git\ncd langextract\n\n# For basic installation:\npip install -e .\n\n# For development (includes linting tools):\npip install -e \".[dev]\"\n\n# For testing (includes pytest):\npip install -e \".[test]\"\n```\n\n### Docker\n\n```bash\ndocker build -t langextract .\ndocker run --rm -e LA",
    "manifest_content": "# Copyright 2025 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n[build-system]\nrequires = [\"setuptools>=67.0.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n\n[project]\nname = \"langextract\"\nversion = \"1.1.1\"\ndescription = \"LangExtract: A library for extracting structured data from language models\"\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\nlicense = \"Apache-2.0\"\nauthors = [\n    {name = \"Akshay Goel\", email = \"goelak@google.com\"}\n]\ndependencies = [\n    \"absl-py>=1.0.0\",\n    \"aiohttp>=3.8.0\",\n    \"async_timeout>=4.0.0\",\n    \"exceptiongroup>=1.1.0\",\n    \"google-genai>=1.39.0\",\n    \"google-cloud-storage>=2.14.0\",\n    \"ml-collections>=0.1.0\",\n    \"more-itertools>=8.0.0\",\n    \"numpy>=1.20.0\",\n    \"pandas>=1.3.0\",\n    \"pydantic>=1.8.0\",\n    \"python-dotenv>=0.19.0\",\n    \"PyYAML>=6.0\",\n    \"regex>=2023.0.0\",\n    \"requests>=2.25.0\",\n    \"tqdm>=4.64.0\",\n    \"typing-extensions>=4.0.0\"\n]\n\n[project.urls]\n\"Homepage\" = \"https://github.com/google/langextract\"\n\"Repository\" = \"https://github.com/google/langextract\"\n\"Documentation\" = \"https://github.com/google/langextract/blob/main/README.md\"\n\"Bug Tracker\" = \"https://github.com/google/langextract/issues\"\n\"Changelog\" = \"https://github.com/google/langextract/releases\"\n\"DOI\" = \"https://doi.org/10.5281/zenodo.17015089\"\n\n[project.optional-dependencies]\nopenai = [\"openai>=1.50.0\"]\nall = [\"openai>=1.50.0\"]\ndev = [\n    \"pyink~=24.3.0\",\n    \"isort>=5.13.0\",\n    \"pylint>=3.0.0\",\n    \"pytype>=2024.10.11\",\n    \"tox>=4.0.0\",\n    \"import-linter>=2.0\",\n    \"pre-commit>=3.5.0\",\n    \"types-regex>=2023.0.0\"\n]\ntest = [\n    \"pytest>=7.4.0\",\n    \"tomli>=2.0.0\"\n]\nnotebook = [\n    \"ipython>=7.0.0\",\n    \"notebook>=6.0.0\"\n]\n\n[tool.setuptools]\npackages = [\n    \"langextract\",\n    \"langextract._compat\",\n    \"langextract.core\",\n    \"langextract.providers\",\n    \"langextract.providers.schemas\"\n]\ninclude-package-data = true\n\n[tool.setuptools.package-data]\nlangextract = [\"py.typed\"]\n\n# Provider discovery mechanism for built-in and third-party providers\n[project.entry-points.\"langextract.providers\"]\ngemini = \"langextract.providers.gemini:GeminiLanguageModel\"\nollama = \"langextract.providers.ollama:OllamaLanguageModel\"\nopenai = \"langextract.providers.openai:OpenAILanguageModel\"\n\n[tool.setuptools.exclude-package-data]\n\"*\" = [\n    \"docs*\",\n    \"tests*\",\n    \"kokoro*\",\n    \"*.gif\",\n    \"*.svg\",\n]\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = \"*_test.py\"\npython_classes = \"Test*\"\npython_functions = \"test_*\"\n# Show extra test summary info\naddopts = \"-ra\"\nmarkers = [\n    \"live_api: marks tests as requiring live API access\",\n    \"requires_pip: marks tests that perform pip install/uninstall operations\",\n    \"integration: marks integration tests that test multiple components together\",\n]\n\n[tool.pyink]\n# Configuration for Google's style guide\nline-length = 80\nunstable = true\npyink-indentation = 2\npyink-use-majority-quotes = true\n\n[tool.isort]\n# Configuration for Google's style guide\nprofile = \"google\"\nline_length = 80\nforce_sort_within_sections = true\n# Allow multiple imports on one line for these modules\nsingle_line_exclusions = [\"typing\", \"typing_extensions\", \"collections.abc\"]\n\n[tool.importlinter]\nroot_package = \"langextract\"\n\n\n[[tool.importlinter.contracts]]\nname = \"Providers must not import inference\"\ntype = \"forbidden\"\nsource_modules = [\"langextract.providers\"]\nforbidden_modules = [\"langextract.inference\"]\n\n[[tool.importlinter.contracts]]\nname = \"Core must not import providers\"\ntype = \"forbidden\"\nsource_modules = [\"langextract.core\"]\nforbidden_modules = [\"langextract.providers\"]\n\n[[tool.importlinter.contracts]]\nname = \"Core must not import high-level modules\"\ntype = \"forbidden\"\nsource_modules = [\"langextract.core\"]\nforbidden_modules = [\n  \"langextract.annotation\",\n  \"langextract.chunking\",\n  \"langextract.prompting\",\n  \"langextract.resolver\",\n]\n"
  },
  {
    "full_name": "ChromeDevTools/chrome-devtools-mcp",
    "name": "chrome-devtools-mcp",
    "description": "Chrome DevTools for coding agents",
    "language": "TypeScript",
    "today_stars": "436",
    "total_stars": "24550",
    "metadata": {
      "topics": [
        "browser",
        "chrome",
        "chrome-devtools",
        "debugging",
        "devtools",
        "mcp",
        "mcp-server",
        "puppeteer"
      ],
      "license": "Apache-2.0",
      "forks": 1460,
      "open_issues": 74,
      "created_at": "2025-09-11",
      "updated_at": "2026-02-13",
      "homepage": "https://npmjs.org/package/chrome-devtools-mcp",
      "default_branch": "main",
      "size_kb": 3705,
      "watchers": 94,
      "archived": false
    },
    "readme_content": "# Chrome DevTools MCP\n\n[![npm chrome-devtools-mcp package](https://img.shields.io/npm/v/chrome-devtools-mcp.svg)](https://npmjs.org/package/chrome-devtools-mcp)\n\n`chrome-devtools-mcp` lets your coding agent (such as Gemini, Claude, Cursor or Copilot)\ncontrol and inspect a live Chrome browser. It acts as a Model-Context-Protocol\n(MCP) server, giving your AI coding assistant access to the full power of\nChrome DevTools for reliable automation, in-depth debugging, and performance analysis.\n\n## [Tool reference](./docs/tool-reference.md) | [Changelog](./CHANGELOG.md) | [Contributing](./CONTRIBUTING.md) | [Troubleshooting](./docs/troubleshooting.md) | [Design Principles](./docs/design-principles.md)\n\n## Key features\n\n- **Get performance insights**: Uses [Chrome\n  DevTools](https://github.com/ChromeDevTools/devtools-frontend) to record\n  traces and extract actionable performance insights.\n- **Advanced browser debugging**: Analyze network requests, take screenshots and\n  check browser console messages (with source-mapped stack traces).\n- **Reliable automation**. Uses\n  [puppeteer](https://github.com/puppeteer/puppeteer) to automate actions in\n  Chrome and automatically wait for action results.\n\n## Disclaimers\n\n`chrome-devtools-mcp` exposes content of the browser instance to the MCP clients\nallowing them to inspect, debug, and modify any data in the browser or DevTools.\nAvoid sharing sensitive or personal information that you don't want to share with\nMCP clients.\n\nPerformance tools may send trace URLs to the Google CrUX API to fetch real-user\nexperience data. This helps provide a holistic performance picture by\npresenting field data alongside lab data. This data is collected by the [Chrome\nUser Experience Report (CrUX)](https://developer.chrome.com/docs/crux). To disable\nthis, run with the `--no-performance-crux` flag.\n\n## **Usage statistics**\n\nGoogle collects usage statistics (such as tool invocation success rates, latency, and environment information) to improve the reliability and performance of Chrome DevTools MCP.\n\nData collection is **enabled by default**. You can opt-out by passing the `--no-usage-statistics` flag when starting the server:\n\n```json\n\"args\": [\"-y\", \"chrome-devtools-mcp@latest\", \"--no-usage-statistics\"]\n```\n\nGoogle handles this data in accordance with the [Google Privacy Policy](https://policies.google.com/privacy).\n\nGoogle's collection of usage statistics for Chrome DevTools MCP is independent from the Chrome browser's usage statistics. Opting out of Chrome metrics does not automatically opt you out of this tool, and vice-versa.\n\nCollection is disabled if CHROME_DEVTOOLS_MCP_NO_USAGE_STATISTICS or CI env variables are set.\n\n## Requirements\n\n- [Node.js](https://nodejs.org/) v20.19 or a newer [latest maintenance LTS](https://github.com/nodejs/Release#release-schedule) version.\n- [Chrome](https://www.google.com/chrome/) current stable version or newer.\n- [npm](https://www.npmjs.com/).\n\n## Getting started\n\nAdd the following config to your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"chrome-devtools-mcp@latest\"]\n    }\n  }\n}\n```\n\n> [!NOTE]  \n> Using `chrome-devtools-mcp@latest` ensures that your MCP client will always use the latest version of the Chrome DevTools MCP server.\n\n### MCP Client configuration\n\n<details>\n  <summary>Amp</summary>\n  Follow https://ampcode.com/manual#mcp and use the config provided above. You can also install the Chrome DevTools MCP server using the CLI:\n\n```bash\namp mcp add chrome-devtools -- npx chrome-devtools-mcp@latest\n```\n\n</details>\n\n<details>\n  <summary>Antigravity</summary>\n\nTo use the Chrome DevTools MCP server follow the instructions from <a href=\"https://antigravity.google/docs/mcp\">Antigravity's docs</a> to install a custom MCP server. Add the following config to the MCP servers config:\n\n```bash\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"chrome-devtools-mcp@latest\",\n        \"--browser-url=http://127.0.0.1:9222\",\n        \"-y\"\n      ]\n    }\n  }\n}\n```\n\nThis will make the Chrome DevTools MCP server automatically connect to the browser that Antigravity is using. If you are not using port 9222, make sure to adjust accordingly.\n\nChrome DevTools MCP will not start the browser instance automatically using this approach as as the Chrome DevTools MCP server runs in Antigravity's built-in browser. If the browser is not already running, you have to start it first by clicking the Chrome icon at the top right corner.\n\n</details>\n\n<details>\n  <summary>Claude Code</summary>\n\n**Install via CLI (MCP only)**\n\nUse the Claude Code CLI to add the Chrome DevTools MCP server (<a href=\"https://code.claude.com/docs/en/mcp\">guide</a>):\n\n```bash\nclaude mcp add chrome-devtools --scope user npx chrome-devtools-mcp@latest\n```\n\n**Install as a Plugin (MCP + Skills)**\n\nTo install Chrome DevTools MCP with skills, add the marketplace registry in Claude Code:\n\n```sh\n/plugin marketplace add ChromeDevTools/chrome-devtools-mcp\n```\n\nThen, install the plugin:\n\n```sh\n/plugin install chrome-devtools-mcp\n```\n\nRestart Claude Code to have the MCP server and skills load (check with `/skills`).\n\n</details>\n\n<details>\n  <summary>Cline</summary>\n  Follow https://docs.cline.bot/mcp/configuring-mcp-servers and use the config provided above.\n</details>\n\n<details>\n  <summary>Codex</summary>\n  Follow the <a href=\"https://github.com/openai/codex/blob/main/docs/advanced.md#model-context-protocol-mcp\">configure MCP guide</a>\n  using the standard config from above. You can also install the Chrome DevTools MCP server using the Codex CLI:\n\n```bash\ncodex mcp add chrome-devtools -- npx chrome-devtools-mcp@latest\n```\n\n**On Windows 11**\n\nConfigure the Chrome install location and increase the startup timeout by updating `.codex/config.toml` and adding the following `env` and `startup_timeout_ms` parameters:\n\n```\n[mcp_servers.chrome-devtools]\ncommand = \"cmd\"\nargs = [\n    \"/c\",\n    \"npx\",\n    \"-y\",\n    \"chrome-devtools-mcp@latest\",\n]\nenv = { SystemRoot=\"C:\\\\Windows\", PROGRAMFILES=\"C:\\\\Program Files\" }\nstartup_timeout_ms = 20_000\n```\n\n</details>\n\n<details>\n  <summary>Copilot CLI</summary>\n\nStart Copilot CLI:\n\n```\ncopilot\n```\n\nStart the dialog to add a new MCP server by running:\n\n```\n/mcp add\n```\n\nConfigure the following fields and press `CTRL+S` to save the configuration:\n\n- **Server name:** `chrome-devtools`\n- **Server Type:** `[1] Local`\n- **Command:** `npx -y chrome-devtools-mcp@latest`\n\n</details>\n\n<details>\n  <summary>Copilot / VS Code</summary>\n\n**Click the button to install:**\n\n[<img src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Server&color=0098FF\" alt=\"Install in VS Code\">](https://vscode.dev/redirect/mcp/install?name=io.github.ChromeDevTools%2Fchrome-devtools-mcp&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22chrome-devtools-mcp%22%5D%2C%22env%22%3A%7B%7D%7D)\n\n[<img src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Server&color=24bfa5\" alt=\"Install in VS Code Insiders\">](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522io.github.ChromeDevTools%252Fchrome-devtools-mcp%2522%252C%2522config%2522%253A%257B%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522chrome-devtools-mcp%2522%255D%252C%2522env%2522%253A%257B%257D%257D%257D)\n\n**Or install manually:**\n\nFollow the MCP install <a href=\"https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server\">guide</a>,\nwith the standard config from above. You can also install the Chrome DevTools MCP server using the VS Code CLI:\n\n```bash\ncode --add-mcp '{\"name\":\"io.github.ChromeDevTools/chrome-devtools-mcp\",\"command\":\"npx\",\"args\":[\"-y\",\"chrome-devtools-mcp\"],\"env\":{}}'\n```\n\n</details>\n\n<details>\n  <summary>Cursor</summary>\n\n**Click the button to install:**\n\n[<img src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\">](https://cursor.com/en/install-mcp?name=chrome-devtools&config=eyJjb21tYW5kIjoibnB4IC15IGNocm9tZS1kZXZ0b29scy1tY3BAbGF0ZXN0In0%3D)\n\n**Or install manually:**\n\nGo to `Cursor Settings` -> `MCP` -> `New MCP Server`. Use the config provided above.\n\n</details>\n\n<details>\n  <summary>Factory CLI</summary>\nUse the Factory CLI to add the Chrome DevTools MCP server (<a href=\"https://docs.factory.ai/cli/configuration/mcp\">guide</a>):\n\n```bash\ndroid mcp add chrome-devtools \"npx -y chrome-devtools-mcp@latest\"\n```\n\n</details>\n\n<details>\n  <summary>Gemini CLI</summary>\nInstall the Chrome DevTools MCP server using the Gemini CLI.\n\n**Project wide:**\n\n```bash\n# Either MCP only:\ngemini mcp add chrome-devtools npx chrome-devtools-mcp@latest\n# Or as a Gemini extension (MCP+Skills):\ngemini extensions install --auto-update https://github.com/ChromeDevTools/chrome-devtools-mcp\n```\n\n**Globally:**\n\n```bash\ngemini mcp add -s user chrome-devtools npx chrome-devtools-mcp@latest\n```\n\nAlternatively, follow the <a href=\"https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#how-to-set-up-your-mcp-server\">MCP guide</a> and use the standard config from above.\n\n</details>\n\n<details>\n  <summary>Gemini Code Assist</summary>\n  Follow the <a href=\"https://cloud.google.com/gemini/docs/codeassist/use-agentic-chat-pair-programmer#configure-mcp-servers\">configure MCP guide</a>\n  using the standard config from above.\n</details>\n\n<details>\n  <summary>JetBrains AI Assistant & Junie</summary>\n\nGo to `Settings | Tools | AI Assistant | Model Context Protocol (MCP)` -> `Add`. Use the config provided above.\nThe same way chrome-devtools-mcp can be configured for JetBrains Junie in `Settings | Tools | Junie | MCP Settings` -> `Add`. Use the config provided above.\n\n</details>\n\n<details>\n  <summary>Kiro</summary>\n\nIn **Kiro Settings**, go to `Configure MCP` > `Open Workspace or User MCP Config` > Use the configuration snippet provided above.\n\nOr, from the",
    "manifest_content": "{\n  \"name\": \"chrome-devtools-mcp\",\n  \"version\": \"0.17.0\",\n  \"description\": \"MCP server for Chrome DevTools\",\n  \"type\": \"module\",\n  \"bin\": \"./build/src/index.js\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"clean\": \"node -e \\\"require('fs').rmSync('build', {recursive: true, force: true})\\\"\",\n    \"bundle\": \"npm run clean && npm run build && rollup -c rollup.config.mjs && node -e \\\"require('fs').rmSync('build/node_modules', {recursive: true, force: true})\\\"\",\n    \"build\": \"tsc && node --experimental-strip-types --no-warnings=ExperimentalWarning scripts/post-build.ts\",\n    \"typecheck\": \"tsc --noEmit\",\n    \"format\": \"eslint --cache --fix . && prettier --write --cache .\",\n    \"check-format\": \"eslint --cache . && prettier --check --cache .;\",\n    \"docs\": \"npm run build && npm run docs:generate && npm run format\",\n    \"docs:generate\": \"node --experimental-strip-types scripts/generate-docs.ts\",\n    \"start\": \"npm run build && node build/src/index.js\",\n    \"start-debug\": \"DEBUG=mcp:* DEBUG_COLORS=false npm run build && node build/src/index.js\",\n    \"test\": \"npm run build && node scripts/test.mjs\",\n    \"test:no-build\": \"node scripts/test.mjs\",\n    \"test:only\": \"npm run build && node scripts/test.mjs --test-only\",\n    \"test:update-snapshots\": \"npm run build && node scripts/test.mjs --test-update-snapshots\",\n    \"prepare\": \"node --experimental-strip-types scripts/prepare.ts\",\n    \"verify-server-json-version\": \"node --experimental-strip-types scripts/verify-server-json-version.ts\",\n    \"eval\": \"npm run build && CHROME_DEVTOOLS_MCP_NO_USAGE_STATISTICS=true node --experimental-strip-types scripts/eval_gemini.ts\",\n    \"count-tokens\": \"node --experimental-strip-types scripts/count_tokens.ts\"\n  },\n  \"files\": [\n    \"build/src\",\n    \"build/node_modules\",\n    \"LICENSE\",\n    \"!*.tsbuildinfo\"\n  ],\n  \"repository\": \"ChromeDevTools/chrome-devtools-mcp\",\n  \"author\": \"Google LLC\",\n  \"license\": \"Apache-2.0\",\n  \"bugs\": {\n    \"url\": \"https://github.com/ChromeDevTools/chrome-devtools-mcp/issues\"\n  },\n  \"homepage\": \"https://github.com/ChromeDevTools/chrome-devtools-mcp#readme\",\n  \"mcpName\": \"io.github.ChromeDevTools/chrome-devtools-mcp\",\n  \"devDependencies\": {\n    \"@eslint/js\": \"^9.35.0\",\n    \"@google/genai\": \"^1.37.0\",\n    \"@modelcontextprotocol/sdk\": \"1.26.0\",\n    \"@rollup/plugin-commonjs\": \"^29.0.0\",\n    \"@rollup/plugin-json\": \"^6.1.0\",\n    \"@rollup/plugin-node-resolve\": \"^16.0.3\",\n    \"@stylistic/eslint-plugin\": \"^5.4.0\",\n    \"@types/debug\": \"^4.1.12\",\n    \"@types/filesystem\": \"^0.0.36\",\n    \"@types/node\": \"^25.0.0\",\n    \"@types/sinon\": \"^21.0.0\",\n    \"@types/yargs\": \"^17.0.33\",\n    \"@typescript-eslint/eslint-plugin\": \"^8.43.0\",\n    \"@typescript-eslint/parser\": \"^8.43.0\",\n    \"chrome-devtools-frontend\": \"1.0.1581449\",\n    \"core-js\": \"3.48.0\",\n    \"debug\": \"4.4.3\",\n    \"eslint\": \"^9.35.0\",\n    \"eslint-import-resolver-typescript\": \"^4.4.4\",\n    \"eslint-plugin-import\": \"^2.32.0\",\n    \"globals\": \"^17.0.0\",\n    \"prettier\": \"^3.6.2\",\n    \"puppeteer\": \"24.37.2\",\n    \"rollup\": \"4.57.1\",\n    \"rollup-plugin-cleanup\": \"^3.2.1\",\n    \"rollup-plugin-license\": \"^3.6.0\",\n    \"sinon\": \"^21.0.0\",\n    \"typescript\": \"^5.9.2\",\n    \"typescript-eslint\": \"^8.43.0\",\n    \"yargs\": \"18.0.0\"\n  },\n  \"engines\": {\n    \"node\": \"^20.19.0 || ^22.12.0 || >=23\"\n  }\n}\n"
  },
  {
    "full_name": "microsoft/PowerToys",
    "name": "PowerToys",
    "description": "Microsoft PowerToys is a collection of utilities that supercharge productivity and customization on Windows",
    "language": "C#",
    "today_stars": "316",
    "total_stars": "129736",
    "metadata": {
      "topics": [
        "advanced-paste",
        "color-picker",
        "command-palette",
        "desktop",
        "fancyzones",
        "keyboard-manager",
        "microsoft-powertoys",
        "powerrename",
        "powertoys",
        "windows",
        "windows-10",
        "windows-11"
      ],
      "license": "MIT",
      "forks": 7711,
      "open_issues": 8134,
      "created_at": "2019-05-01",
      "updated_at": "2026-02-13",
      "homepage": "",
      "default_branch": "main",
      "size_kb": 482938,
      "watchers": 1165,
      "archived": false
    },
    "readme_content": "<p align=\"center\">\n    <picture>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"./doc/images/readme/pt-hero.light.png\" />\n      <img src=\"./doc/images/readme/pt-hero.dark.png\" />\n  </picture>\n</p>\n<h1 align=\"center\">\n  <span>Microsoft PowerToys</span>\n</h1>\n<p align=\"center\">\n  <span align=\"center\">Microsoft PowerToys is a collection of utilities that help you customize Windows and streamline everyday tasks.</span>\n</p>\n<h3 align=\"center\">\n  <a href=\"#-installation\">Installation</a>\n  <span> ¬∑ </span>\n  <a href=\"https://aka.ms/powertoys-docs\">Documentation</a>\n  <span> ¬∑ </span>\n  <a href=\"https://aka.ms/powertoys-releaseblog\">Blog</a>\n  <span> ¬∑ </span>\n  <a href=\"#-whats-new\">Release notes</a>\n</h3>\n<br/><br/>\n\n## üî® Utilities\n\nPowerToys includes over 25 utilities to help you customize and optimize your Windows experience:\n\n|   |   |   |\n|---|---|---|\n| [<img src=\"doc/images/icons/AdvancedPaste.png\" alt=\"Advanced Paste icon\" height=\"16\"> Advanced Paste](https://aka.ms/PowerToysOverview_AdvancedPaste) | [<img src=\"doc/images/icons/Always%20On%20Top.png\" alt=\"Always on Top icon\" height=\"16\"> Always on Top](https://aka.ms/PowerToysOverview_AoT) | [<img src=\"doc/images/icons/Awake.png\" alt=\"Awake icon\" height=\"16\"> Awake](https://aka.ms/PowerToysOverview_Awake) |\n| [<img src=\"doc/images/icons/Color%20Picker.png\" alt=\"Color Picker icon\" height=\"16\"> Color Picker](https://aka.ms/PowerToysOverview_ColorPicker) | [<img src=\"doc/images/icons/Command%20Not%20Found.png\" alt=\"Command Not Found icon\" height=\"16\"> Command Not Found](https://aka.ms/PowerToysOverview_CmdNotFound) | [<img src=\"doc/images/icons/Command Palette.png\" alt=\"Command Palette icon\" height=\"16\"> Command Palette](https://aka.ms/PowerToysOverview_CmdPal) |\n| [<img src=\"doc/images/icons/Crop%20And%20Lock.png\" alt=\"Crop and Lock icon\" height=\"16\"> Crop And Lock](https://aka.ms/PowerToysOverview_CropAndLock) | [<img src=\"doc/images/icons/Environment%20Manager.png\" alt=\"Environment Variables icon\" height=\"16\"> Environment Variables](https://aka.ms/PowerToysOverview_EnvironmentVariables) | [<img src=\"doc/images/icons/FancyZones.png\" alt=\"FancyZones icon\" height=\"16\"> FancyZones](https://aka.ms/PowerToysOverview_FancyZones) |\n| [<img src=\"doc/images/icons/File%20Explorer%20Preview.png\" alt=\"File Explorer Add-ons icon\" height=\"16\"> File Explorer Add-ons](https://aka.ms/PowerToysOverview_FileExplorerAddOns) | [<img src=\"doc/images/icons/File%20Locksmith.png\" alt=\"File Locksmith icon\" height=\"16\"> File Locksmith](https://aka.ms/PowerToysOverview_FileLocksmith) | [<img src=\"doc/images/icons/Host%20File%20Editor.png\" alt=\"Hosts File Editor icon\" height=\"16\"> Hosts File Editor](https://aka.ms/PowerToysOverview_HostsFileEditor) |\n| [<img src=\"doc/images/icons/Image%20Resizer.png\" alt=\"Image Resizer icon\" height=\"16\"> Image Resizer](https://aka.ms/PowerToysOverview_ImageResizer) | [<img src=\"doc/images/icons/Keyboard%20Manager.png\" alt=\"Keyboard Manager icon\" height=\"16\"> Keyboard Manager](https://aka.ms/PowerToysOverview_KeyboardManager) | [<img src=\"doc/images/icons/Light Switch.png\" alt=\"Light Switch icon\" height=\"16\"> Light Switch](https://aka.ms/PowerToysOverview_LightSwitch) |\n| [<img src=\"doc/images/icons/Find My Mouse.png\" alt=\"Mouse Utilities icon\" height=\"16\"> Mouse Utilities](https://aka.ms/PowerToysOverview_MouseUtilities) | [<img src=\"doc/images/icons/MouseWithoutBorders.png\" alt=\"Mouse Without Borders icon\" height=\"16\"> Mouse Without Borders](https://aka.ms/PowerToysOverview_MouseWithoutBorders) | [<img src=\"doc/images/icons/NewPlus.png\" alt=\"New+ icon\" height=\"16\"> New+](https://aka.ms/PowerToysOverview_NewPlus) |\n| [<img src=\"doc/images/icons/Peek.png\" alt=\"Peek icon\" height=\"16\"> Peek](https://aka.ms/PowerToysOverview_Peek) | [<img src=\"doc/images/icons/PowerRename.png\" alt=\"PowerRename icon\" height=\"16\"> PowerRename](https://aka.ms/PowerToysOverview_PowerRename) | [<img src=\"doc/images/icons/PowerToys%20Run.png\" alt=\"PowerToys Run icon\" height=\"16\"> PowerToys Run](https://aka.ms/PowerToysOverview_PowerToysRun) |\n| [<img src=\"doc/images/icons/PowerAccent.png\" alt=\"Quick Accent icon\" height=\"16\"> Quick Accent](https://aka.ms/PowerToysOverview_QuickAccent) | [<img src=\"doc/images/icons/Registry%20Preview.png\" alt=\"Registry Preview icon\" height=\"16\"> Registry Preview](https://aka.ms/PowerToysOverview_RegistryPreview) | [<img src=\"doc/images/icons/MeasureTool.png\" alt=\"Screen Ruler icon\" height=\"16\"> Screen Ruler](https://aka.ms/PowerToysOverview_ScreenRuler) |\n| [<img src=\"doc/images/icons/Shortcut%20Guide.png\" alt=\"Shortcut Guide icon\" height=\"16\"> Shortcut Guide](https://aka.ms/PowerToysOverview_ShortcutGuide) | [<img src=\"doc/images/icons/PowerOCR.png\" alt=\"Text Extractor icon\" height=\"16\"> Text Extractor](https://aka.ms/PowerToysOverview_TextExtractor) | [<img src=\"doc/images/icons/Workspaces.png\" alt=\"Workspaces icon\" height=\"16\"> Workspaces](https://aka.ms/PowerToysOverview_Workspaces) |\n| [<img src=\"doc/images/icons/ZoomIt.png\" alt=\"ZoomIt icon\" height=\"16\"> ZoomIt](https://aka.ms/PowerToysOverview_ZoomIt) |   |   |\n\n\n## üìã Installation\n\nFor detailed installation instructions and system requirements, visit the [installation docs](https://learn.microsoft.com/windows/powertoys/install). \n\nBut to get started quickly, choose one of the installation methods below:\n<br/><br/>\n<details open>\n<summary><strong>Download .exe from GitHub</strong></summary>\n<br/>\nGo to the <a href=\"https://aka.ms/installPowerToys\">PowerToys GitHub releases</a>, click Assets to reveal the downloads, and choose the installer that matches your architecture and install scope. For most devices, that's the x64 per-user installer.\n\n<!-- items that need to be updated release to release -->\n[github-next-release-work]: https://github.com/microsoft/PowerToys/issues?q=is%3Aissue+milestone%3A%22PowerToys+0.98%22\n[github-current-release-work]: https://github.com/microsoft/PowerToys/issues?q=is%3Aissue+milestone%3A%22PowerToys+0.97%22\n[ptUserX64]: https://github.com/microsoft/PowerToys/releases/download/v0.97.1/PowerToysUserSetup-0.97.1-x64.exe \n[ptUserArm64]: https://github.com/microsoft/PowerToys/releases/download/v0.97.1/PowerToysUserSetup-0.97.1-arm64.exe \n[ptMachineX64]: https://github.com/microsoft/PowerToys/releases/download/v0.97.1/PowerToysSetup-0.97.1-x64.exe \n[ptMachineArm64]: https://github.com/microsoft/PowerToys/releases/download/v0.97.1/PowerToysSetup-0.97.1-arm64.exe\n \n|  Description   | Filename |\n|----------------|----------|\n| Per user - x64       | [PowerToysUserSetup-0.97.1-x64.exe][ptUserX64] |\n| Per user - ARM64     | [PowerToysUserSetup-0.97.1-arm64.exe][ptUserArm64] |\n| Machine wide - x64   | [PowerToysSetup-0.97.1-x64.exe][ptMachineX64] |\n| Machine wide - ARM64 | [PowerToysSetup-0.97.1-arm64.exe][ptMachineArm64] |\n\n</details>\n\n<details>\n<summary><strong>Microsoft Store</strong></summary>\n<br/>\nYou can easily install PowerToys from the Microsoft Store:\n<p>\n  <a style=\"text-decoration:none\" href=\"https://aka.ms/getPowertoys\">\n    <picture>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"doc/images/readme/StoreBadge-dark.png\" width=\"148\" />\n      <img src=\"doc/images/readme/StoreBadge-light.png\" width=\"148\" />\n  </picture></a>\n</p>\n</details>\n\n<details>\n<summary><strong>WinGet</strong></summary>\n<br/>\nDownload PowerToys from <a href=\"https://github.com/microsoft/winget-cli#installing-the-client\">WinGet</a>. Updating PowerToys via winget will respect the current PowerToys installation scope. To install PowerToys, run the following command from the command line / PowerShell:\n\n*User scope installer [default]*\n```powershell\nwinget install Microsoft.PowerToys -s winget\n```\n\n*Machine-wide scope installer*\n```powershell\nwinget install --scope machine Microsoft.PowerToys -s winget\n```\n</details>\n\n<details>\n<summary><strong>Other methods</strong></summary>\n<br/>\nThere are <a href=\"https://learn.microsoft.com/windows/powertoys/install#community-driven-install-tools\">community driven install methods</a> such as Chocolatey and Scoop. If these are your preferred install solutions, you can find the install instructions there.\n</details>\n\n## ‚ú® What's new\n\n**Version 0.97.2 (Feb 2026)**\n\nThis patch release fixes several important stability issues identified in v0.97.0 based on incoming reports. Check out the [v0.97.0](https://github.com/microsoft/PowerToys/releases/tag/v0.97.0) notes for the full list of changes.\n\n## Advanced Paste\n- #45207 Fixed a crash in the Advanced Paste settings page caused by null values during JSON deserialization.\n\n## Color Picker\n- #45367 Fixed contrast issue in Color picker UI.\n\n## Command Palette\n- #45194 Fixed an issue where some Command Palette PowerToys Extension strings were not localised.\n\n## Cursor Wrap\n- #45210 Fixed \"Automatically activate on utility startup\" setting not persisting when disabled. Thanks [@ThanhNguyxn](https://github.com/ThanhNguyxn)!\n- #45303 Added option to disable Cursor Wrapping when only a single monitor is connected. Thanks [@mikehall-ms](https://github.com/mikehall-ms)!\n\n## Image Resizer\n- #45184 Fixed Image Resizer not working after upgrading PowerToys on Windows 10 by properly cleaning up legacy sparse app packages.\n\n## LightSwitch\n- #45304 Fixed Light Switch startup logic to correctly apply the appropriate theme on launch.\n\n## Workspaces\n- #45183 Fixed overlay positioning issue in workspace snapshot draw caused by DPI-aware coordinate mismatch.\n\n## Quick Access and Measure Tool\n- #45443 Fixed crash related to `IsShownInSwitchers` property when Explorer is not running.\n\n**Version 0.97.1 (January 2026)**\n\n**Highlights**\n\n### Advanced Paste\n- #44862: Fixed Settings UI advanced paste page crash by using correct settings repository for null checking.\n\n### Command Palette\n- #44886: Fixed personalization section not appearing by using latest MSIX for installation.\n- #44938: Fixed loading of icons from internet shortcuts. Thanks [@jiripolasek](https://github.com/jiripolasek)!\n- #45076: Fixed potential dea"
  },
  {
    "full_name": "iOfficeAI/AionUi",
    "name": "AionUi",
    "description": "Free, local, open-source 24/7 Cowork and OpenClaw for Gemini CLI, Claude Code, Codex, OpenCode, Qwen Code, Goose CLI, Auggie, and more | üåü Star if you like it!",
    "language": "TypeScript",
    "today_stars": "271",
    "total_stars": "15613",
    "metadata": {
      "topics": [
        "acp",
        "ai",
        "ai-agent",
        "chat",
        "chatbot",
        "claude-code",
        "clawd",
        "clawdbot",
        "codex",
        "cowork",
        "gemini",
        "gemini-cli",
        "llm",
        "nano-banana",
        "office",
        "openclaw",
        "opencode",
        "skills",
        "webui"
      ],
      "license": "Apache-2.0",
      "forks": 1183,
      "open_issues": 106,
      "created_at": "2025-08-07",
      "updated_at": "2026-02-13",
      "homepage": "https://www.aionui.com",
      "default_branch": "main",
      "size_kb": 382003,
      "watchers": 72,
      "archived": false
    },
    "readme_content": "<p align=\"center\">\n  <img src=\"./resources/aionui-banner-1.png\" alt=\"AionUi - Cowork with Your CLI AI Agent\" width=\"100%\">\n</p>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/github/v/release/iOfficeAI/AionUi?style=flat-square&color=32CD32\" alt=\"Version\">\n  &nbsp;\n  <img src=\"https://img.shields.io/badge/license-Apache--2.0-32CD32?style=flat-square&logo=apache&logoColor=white\" alt=\"License\">\n  &nbsp;\n  <img src=\"https://img.shields.io/badge/platform-macOS%20%7C%20Windows%20%7C%20Linux-6C757D?style=flat-square&logo=linux&logoColor=white\" alt=\"Platform\">\n</p>\n\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/15423\" target=\"_blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/15423\" alt=\"GitHub Trending\" height=\"80\">\n  </a>\n</p>\n\n---\n\n<p align=\"center\">\n  <strong>üöÄ Cowork with Your AI, Gemini CLI, Claude Code, Codex, Qwen Code, Goose CLI, OpenClaw, Auggie, and more</strong><br>\n  <em>User-friendly | Visual graphical interface | Multi-model support | Local data security</em>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/iOfficeAI/AionUi/releases\">\n    <img src=\"https://img.shields.io/badge/‚¨áÔ∏è%20Download%20Now-Latest%20Release-32CD32?style=for-the-badge&logo=github&logoColor=white\" alt=\"Download Latest Release\" height=\"50\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <strong>English</strong> | <a href=\"./readme_ch.md\">ÁÆÄ‰Ωì‰∏≠Êñá</a> | <a href=\"./readme_tw.md\">ÁπÅÈ´î‰∏≠Êñá</a> | <a href=\"./readme_jp.md\">Êó•Êú¨Ë™û</a> | <a href=\"./readme_ko.md\">ÌïúÍµ≠Ïñ¥</a> | <a href=\"./readme_es.md\">Espa√±ol</a> | <a href=\"./readme_pt.md\">Portugu√™s</a> | <a href=\"./readme_tr.md\">T√ºrk√ße</a> | <a href=\"https://www.aionui.com\" target=\"_blank\">Official Website</a> | <a href=\"https://twitter.com/AionUI\" target=\"_blank\">Twitter</a>\n</p>\n\n<p align=\"center\">\n  <strong>üí¨ Community:</strong> <a href=\"https://discord.gg/2QAwJn7Egx\" target=\"_blank\">Discord (English)</a> | <a href=\"./resources/wechat-group-3.png\" target=\"_blank\">ÂæÆ‰ø° (‰∏≠ÊñáÁæ§)</a>\n</p>\n\n---\n\n## üìã Quick Navigation\n\n<p align=\"center\">\n\n[‚ú® What Can AionUi Do?](#‚ú®-what-can-aionui-do) ¬∑\n[ü§î Why Choose AionUi?](#ü§î-why-choose-aionui) ¬∑\n[‚ú® Core Features](#‚ú®-core-features) ¬∑\n[üöÄ Quick Start](#üöÄ-quick-start) ¬∑\n[üí¨ Community](#ü§ù-community--support)\n\n</p>\n\n---\n\n## ‚ú® What Can AionUi Do?\n\n<p align=\"center\">\n  <img src=\"./resources/offica-ai BANNER-function.png\" alt=\"AionUi - Cowork with Your CLI AI Agent\" width=\"800\">\n</p>\n\n### ü§ñ **Multi-Agent Mode - Cowork for Your Command-Line AI Tools, Unified Graphical Interface**\n\nAionUi provides a unified graphical interface for your command-line AI tools. Built-in Gemini CLI included, no setup required.\n\n**Supported Tools:** Gemini CLI (built-in) ‚Ä¢ Claude Code ‚Ä¢ CodeX ‚Ä¢ Qwen Code ‚Ä¢ Goose AI ‚Ä¢ OpenClaw ‚Ä¢ Augment Code\n\n<p align=\"center\">\n  <img src=\"./resources/multi-agentÊîØÊåÅopenclaw.gif\" alt=\"OpenClaw Integration in AionUi\" width=\"800\">\n</p>\n\n**Key Features:**\n\n- ‚úÖ **Auto Detection** - Automatically recognizes and integrates local CLI tools\n- ‚úÖ **Unified Interface** - One interface for all your AI tools, no more command line\n- ‚úÖ **Local Storage + Multi-Session** - Conversations saved locally, multiple parallel sessions with independent context\n\n---\n\n### üåê **Access Your AionUi Anywhere**\n\n_Your 7√ó24 hour AI assistant - Access AionUi from any device, anywhere! On business trips, at home, in the office, use your AI tools anytime, anywhere through WebUI or various chat platforms_\n\nAionUi provides multiple remote access methods:\n\n- **üåê WebUI Mode**\n\n  Access AionUi from any device via browser - phone, tablet, computer. Supports LAN, cross-network, and server deployment. You can log in by scanning a QR code or using account password, making it simple and convenient.\n\n  > üí° **Need detailed configuration guide?** Check out [Remote Internet Access Tutorial](https://github.com/iOfficeAI/AionUi/wiki/Remote-Internet-Access-Guide-Chinese)\n\n- **üì± Chat Platform Integration**\n  - **Telegram** - Chat with your AI assistant directly from Telegram on any device. Simple pairing code system for secure access.\n  - **Lark (Feishu)** - Interact with your AI assistant through Feishu bots, supporting enterprise collaboration scenarios.\n  - **Slack** and more platforms coming soon üöß\n\n  > üí° **How to set up:** Go to AionUi Settings ‚Üí WebUI Settings ‚Üí Channel, configure the corresponding Bot Token to get started!\n\n<p align=\"center\">\n  <img src=\"./resources/remote-telegram.png\" alt=\"Access Anywhere - WebUI & Chat Platforms\" width=\"800\">\n</p>\n\n---\n\n### ‚è∞ **Scheduled Tasks - Let AionUi Automate Your Work**\n\n_After setting up scheduled tasks, the AI assistant will automatically execute according to your set time, truly achieving 7√ó24 hours unattended operation_\n\n- **Free Conversation** - Tell AI what to do using natural language, just like chatting normally\n- **Flexible Time Setting** - Daily, weekly, monthly are all possible\n- **Simple and Easy** - Create, modify, enable/disable, delete, view and adjust anytime\n\n<p align=\"center\">\n  <img src=\"./resources/alart-task.png\" alt=\"Scheduled Tasks Demo\" width=\"800\">\n</p>\n\n> üí° **Use Cases:** Scheduled data aggregation, regular report generation, automatic file organization, scheduled reminders, etc.\n\n---\n\n### üìÅ **Smart File Management (AI Cowork)**\n\n_Batch renaming, automatic organization, smart classification, file merging_\n\n- **Auto Organize**: Intelligently identify content and auto-classify, keeping folders tidy.\n- **Efficient Batch**: One-click rename, merge files, say goodbye to tedious manual tasks.\n\n<p align=\"center\">\n  <img src=\"./resources/aionui sort file.gif\" alt=\"Smart File Management Demo\" width=\"800\">\n</p>\n\n---\n\n### üìÑ **Preview Panel - Quickly View AI-Generated Results**\n\n_Supports 9+ formats of visual preview (PDF, Word, Excel, PPT, code, Markdown, images, HTML, Diff, etc.)_\n\n- ‚úÖ **View Results Instantly** - After AI generates files, view preview immediately without switching apps\n- ‚úÖ **Real-time Tracking + Editable** - Automatically tracks file changes, editor and preview sync intelligently; supports real-time editing of Markdown, code, HTML, WYSIWYG\n\n<p align=\"center\">\n  <img src=\"./resources/preview.gif\" alt=\"Preview Panel Demo\" width=\"800\">\n</p>\n\n---\n\n### üé® **AI Image Generation & Editing**\n\n_Intelligent image generation, editing, and recognition, powered by Gemini_\n\n<p align=\"center\">\n  <img src=\"./resources/Image_Generation.gif\" alt=\"AI Image Generation Demo\" width=\"800\">\n</p>\n\n> üí° **Need help setting up free image generation?** [Follow the tutorial to configure image generation models](https://github.com/iOfficeAI/AionUi/wiki/AionUi-Image-Generation-Tool-Model-Configuration-Guide)\n\n---\n\n### ü§ñ **Multi-Model Support**\n\n_Supports mainstream models like Gemini, OpenAI, Claude, Qwen, as well as local models like Ollama, LM Studio. AionUi also supports [NewAPI](https://github.com/QuantumNous/new-api) gateway service(a unified AI model hub that aggregates and distributes various LLMs). Flexibly switch between different models to meet various task requirements._\n\n<p align=\"center\">\n  <img src=\"./resources/llm_newapi.png\" alt=\"Multi-Model Support\" width=\"800\">\n</p>\n\n---\n\n---\n\n### üõ†Ô∏è **AI Assistants & Skills Ecosystem**\n\n_Extensible assistant system with built-in specialized assistants and custom skill support_\n\n<details>\n<summary><strong>üîç Click to explore AI Assistants & Skills ‚ñ∂Ô∏è</strong></summary>\n\n<br>\n\nAionUi includes **10+ professional assistants** with predefined capabilities, extendable through custom skills:\n\n- **ü§ù Cowork** - Autonomous task execution (file operations, document processing, workflow planning)\n- **üìä PPTX Generator** - Generate PPTX presentations\n- **üìÑ PDF to PPT** - Convert PDF to PPT\n- **üéÆ 3D Game** - Single-file 3D game generation\n- **üé® UI/UX Pro Max** - Professional UI/UX design (57 styles, 95 color palettes)\n- **üìã Planning with Files** - File-based planning for complex tasks\n- **üß≠ HUMAN 3.0 Coach** - Personal development coach\n- **üì£ Social Job Publisher** - Job posting and publishing\n- **ü¶û moltbook** - Zero-deployment integration with automatic heartbeat scheduling, activity reporting, and seamless AI agent social networking\n- **üìà Beautiful Mermaid** - Flowcharts, sequence diagrams, and more\n\n**Custom Skills**: Create skills in the `skills/` directory, enable/disable skills for assistants to extend AI capabilities. Built-in skills include `pptx`, `docx`, `pdf`, `xlsx`, `mermaid`, and more.\n\n> üí° Each assistant is defined by a markdown file. Check the `assistant/` directory for examples.\n\n</details>\n\n### üé® **Personalized Interface Customization**\n\n_Customize with your own CSS code, make your interface match your preferences_\n\n<p align=\"center\">\n  <img src=\"./resources/css with skin.gif\" alt=\"CSS Custom Interface Demo\" width=\"800\">\n</p>\n\n- ‚úÖ **Fully Customizable** - Freely customize interface colors, styles, layout through CSS code, create your exclusive experience\n\n---\n\n### üí¨ **Multi-Task Parallel Processing**\n\n_Open multiple conversations, tasks don't get mixed up, independent memory, double efficiency_\n\n<details>\n<summary><strong>üé¨ Click to view demo ‚ñ∂Ô∏è</strong></summary>\n<br>\n<p align=\"center\">\n  <img src=\"./resources/multichat-side-by-side.gif\" alt=\"Conversation Management Demo\" width=\"800\">\n</p>\n</details>\n\n---\n\n## ü§î Why Choose AionUi?\n\n**Just like Claude Cowork makes Claude Code easier to use, AionUi is the Cowork platform for all your command-line AI tools**\n\nWhile command-line tools like Gemini CLI, Claude Code, Codex, Qwen Code are powerful, they share common pain points: conversations can't be saved, single-session limitations, cumbersome file operations, and only support a single model.\n\nAionUi provides unified **Cowork capabilities** for these command-line tools:\n\n- üéØ **Unified Platform** - One interface to manage all command-line AI tools, no switching needed; built-in Gemini CLI, ready to use out of the box and completely free\n- üöÄ **Multi-Tool Support** - Not only supports Claude Code, but also Gemini CLI, Codex, Qwen Code, and more\n- üñ•Ô∏è **Cross-Platform** - Full platform support for mac",
    "manifest_content": "{\n  \"name\": \"AionUi\",\n  \"productName\": \"AionUi\",\n  \"version\": \"1.8.8\",\n  \"description\": \"Transform your command-line AI agent into a modern, efficient AI Chat interface.\",\n  \"main\": \".webpack/main\",\n  \"scripts\": {\n    \"start\": \"node scripts/start-forge.js\",\n    \"cli\": \"node scripts/start-forge.js\",\n    \"webui\": \"npm run cli -- --webui\",\n    \"webui:remote\": \"npm run cli -- --webui --remote\",\n    \"webui:prod\": \"cross-env NODE_ENV=production npm run cli -- --webui\",\n    \"webui:prod:remote\": \"cross-env NODE_ENV=production npm run cli -- --webui --remote\",\n    \"resetpass\": \"npm run cli -- --resetpass\",\n    \"package\": \"electron-forge package\",\n    \"make\": \"electron-forge make\",\n    \"dist\": \"node scripts/build-with-builder.js\",\n    \"dist:mac\": \"node scripts/build-with-builder.js auto --mac\",\n    \"dist:win\": \"node scripts/build-with-builder.js auto --win\",\n    \"dist:linux\": \"node scripts/build-with-builder.js auto --linux\",\n    \"build-mac\": \"node scripts/build-with-builder.js auto --mac --arm64 --x64\",\n    \"build-win\": \"node scripts/build-with-builder.js auto --win\",\n    \"build-deb\": \"node scripts/build-with-builder.js auto --linux\",\n    \"build-mac:arm64\": \"node scripts/build-with-builder.js arm64 --mac --arm64\",\n    \"build-mac:x64\": \"node scripts/build-with-builder.js x64 --mac --x64\",\n    \"build\": \"node scripts/build-with-builder.js auto --mac --arm64 --x64\",\n    \"lint\": \"eslint --ext .ts,.tsx .\",\n    \"lint:fix\": \"eslint --ext .ts,.tsx . --fix\",\n    \"format\": \"prettier --write \\\"src/**/*.{ts,tsx,js,jsx,json,css,md}\\\"\",\n    \"format:check\": \"prettier --check \\\"src/**/*.{ts,tsx,js,jsx,json,css,md}\\\"\",\n    \"test\": \"jest\",\n    \"test:watch\": \"jest --watch\",\n    \"test:coverage\": \"jest --coverage\",\n    \"test:contract\": \"jest tests/contract\",\n    \"test:integration\": \"jest tests/integration\",\n    \"prepare\": \"husky\",\n    \"postinstall\": \"node scripts/postinstall.js\"\n  },\n  \"keywords\": [],\n  \"author\": {\n    \"name\": \"AionUi\",\n    \"email\": \"service@aionui.com\"\n  },\n  \"license\": \"Apache-2.0\",\n  \"devDependencies\": {\n    \"@electron-forge/cli\": \"^7.8.1\",\n    \"@electron-forge/maker-deb\": \"^7.8.3\",\n    \"@electron-forge/maker-dmg\": \"^7.8.1\",\n    \"@electron-forge/maker-rpm\": \"^7.8.1\",\n    \"@electron-forge/maker-squirrel\": \"^7.8.1\",\n    \"@electron-forge/maker-wix\": \"^7.9.0\",\n    \"@electron-forge/maker-zip\": \"^7.8.1\",\n    \"@electron-forge/plugin-auto-unpack-natives\": \"^7.8.1\",\n    \"@electron-forge/plugin-fuses\": \"^7.8.1\",\n    \"@electron-forge/plugin-webpack\": \"^7.8.3\",\n    \"@electron/fuses\": \"^1.8.0\",\n    \"@electron/notarize\": \"^3.1.0\",\n    \"@svgr/webpack\": \"^8.1.0\",\n    \"@types/better-sqlite3\": \"^7.6.13\",\n    \"@types/cookie-parser\": \"^1.4.9\",\n    \"@types/cors\": \"^2.8.19\",\n    \"@types/html-to-text\": \"^9.0.4\",\n    \"@types/node\": \"^24.3.1\",\n    \"@types/qrcode-terminal\": \"^0.12.0\",\n    \"@types/react-dom\": \"^19.1.6\",\n    \"@types/react-syntax-highlighter\": \"^15.5.13\",\n    \"@types/semver\": \"^7.7.1\",\n    \"@types/turndown\": \"^5.0.6\",\n    \"@types/ws\": \"^8.18.1\",\n    \"@typescript-eslint/eslint-plugin\": \"^6.21.0\",\n    \"@typescript-eslint/parser\": \"^6.21.0\",\n    \"@unocss/webpack\": \"^66.3.3\",\n    \"@vercel/webpack-asset-relocator-loader\": \"1.7.3\",\n    \"copy-webpack-plugin\": \"^13.0.1\",\n    \"cross-env\": \"^7.0.3\",\n    \"css-loader\": \"^6.11.0\",\n    \"dotenv\": \"^17.2.1\",\n    \"electron\": \"^37.3.1\",\n    \"electron-builder\": \"^26.6.0\",\n    \"eslint\": \"^8.57.1\",\n    \"eslint-config-prettier\": \"^10.1.8\",\n    \"eslint-plugin-import\": \"^2.32.0\",\n    \"eslint-plugin-prettier\": \"^5.5.4\",\n    \"fork-ts-checker-webpack-plugin\": \"^7.3.0\",\n    \"husky\": \"^9.1.7\",\n    \"jest\": \"^30.1.3\",\n    \"jest-diff\": \"^30.0.4\",\n    \"lint-staged\": \"^16.1.5\",\n    \"mini-css-extract-plugin\": \"^2.9.2\",\n    \"node-loader\": \"^2.1.0\",\n    \"patch-package\": \"^8.0.0\",\n    \"postcss-loader\": \"^8.1.1\",\n    \"prettier\": \"^3.6.2\",\n    \"style-loader\": \"^3.3.4\",\n    \"ts-jest\": \"^29.4.4\",\n    \"ts-loader\": \"^9.5.2\",\n    \"ts-node\": \"^10.9.2\",\n    \"typescript\": \"^5.8.3\",\n    \"unocss\": \"^66.3.3\",\n    \"unocss-preset-extra\": \"^1.0.0\",\n    \"webpack-cli\": \"^6.0.1\"\n  },\n  \"dependencies\": {\n    \"@anthropic-ai/sdk\": \"^0.71.2\",\n    \"@arco-design/web-react\": \"^2.66.1\",\n    \"@aws-sdk/client-bedrock\": \"^3.987.0\",\n    \"@codemirror/lang-css\": \"^6.3.1\",\n    \"@codemirror/lang-json\": \"^6.0.2\",\n    \"@codemirror/lang-markdown\": \"^6.5.0\",\n    \"@floating-ui/react\": \"^0.27.16\",\n    \"@google/genai\": \"^1.16.0\",\n    \"@grammyjs/transformer-throttler\": \"^1.2.1\",\n    \"@icon-park/react\": \"^1.4.2\",\n    \"@larksuiteoapi/node-sdk\": \"^1.58.0\",\n    \"@modelcontextprotocol/sdk\": \"^1.20.0\",\n    \"@monaco-editor/react\": \"^4.7.0\",\n    \"@office-ai/aioncli-core\": \"^0.24.4\",\n    \"@office-ai/platform\": \"^0.3.16\",\n    \"@types/bcryptjs\": \"^2.4.6\",\n    \"@types/jsonwebtoken\": \"^9.0.10\",\n    \"@uiw/codemirror-extensions-langs\": \"^4.25.1\",\n    \"@uiw/react-codemirror\": \"^4.25.2\",\n    \"bcryptjs\": \"^2.4.3\",\n    \"better-sqlite3\": \"^12.4.1\",\n    \"buffer\": \"^6.0.3\",\n    \"classnames\": \"^2.5.1\",\n    \"cookie-parser\": \"^1.4.7\",\n    \"cors\": \"^2.8.5\",\n    \"croner\": \"^9.1.0\",\n   "
  },
  {
    "full_name": "Shubhamsaboo/awesome-llm-apps",
    "name": "awesome-llm-apps",
    "description": "Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.",
    "language": "Python",
    "today_stars": "287",
    "total_stars": "94542",
    "metadata": {
      "topics": [
        "agents",
        "llms",
        "python",
        "rag"
      ],
      "license": "Apache-2.0",
      "forks": 13702,
      "open_issues": 13,
      "created_at": "2024-04-29",
      "updated_at": "2026-02-13",
      "homepage": "https://www.theunwindai.com",
      "default_branch": "main",
      "size_kb": 196517,
      "watchers": 1044,
      "archived": false
    },
    "readme_content": "<p align=\"center\">\n  <a href=\"http://www.theunwindai.com\">\n    <img src=\"docs/banner/unwind_black.png\" width=\"900px\" alt=\"Unwind AI\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.linkedin.com/in/shubhamsaboo/\">\n    <img src=\"https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&style=flat-square\" alt=\"LinkedIn\">\n  </a>\n  <a href=\"https://twitter.com/Saboo_Shubham_\">\n    <img src=\"https://img.shields.io/twitter/follow/Shubham_Saboo\" alt=\"Twitter\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <!-- Keep these links. Translations will automatically update with the README. -->\n  <a href=\"https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de\">Deutsch</a> | \n  <a href=\"https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es\">Espa√±ol</a> | \n  <a href=\"https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr\">fran√ßais</a> | \n  <a href=\"https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja\">Êó•Êú¨Ë™û</a> | \n  <a href=\"https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko\">ÌïúÍµ≠Ïñ¥</a> | \n  <a href=\"https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt\">Portugu√™s</a> | \n  <a href=\"https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru\">–†—É—Å—Å–∫–∏–π</a> | \n  <a href=\"https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh\">‰∏≠Êñá</a>\n</p>\n\n<hr/>\n\n# üåü Awesome LLM Apps\n\nA curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from <img src=\"https://cdn.simpleicons.org/openai\"  alt=\"openai logo\" width=\"25\" height=\"15\">**OpenAI** , <img src=\"https://cdn.simpleicons.org/anthropic\"  alt=\"anthropic logo\" width=\"25\" height=\"15\">**Anthropic**, <img src=\"https://cdn.simpleicons.org/googlegemini\"  alt=\"google logo\" width=\"25\" height=\"18\">**Google**, <img src=\"https://cdn.simpleicons.org/x\"  alt=\"X logo\" width=\"25\" height=\"15\">**xAI** and open-source models like <img src=\"https://cdn.simpleicons.org/alibabacloud\"  alt=\"alibaba logo\" width=\"25\" height=\"15\">**Qwen** or  <img src=\"https://cdn.simpleicons.org/meta\"  alt=\"meta logo\" width=\"25\" height=\"15\">**Llama** that you can run locally on your computer.\n\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/9876\" target=\"_blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/9876\" alt=\"Shubhamsaboo%2Fawesome-llm-apps | Trendshift\" style=\"width: 250px; height: 55px;\" />\n  </a>\n</p>\n\n## ü§î Why Awesome LLM Apps?\n\n- üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.\n- üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP & RAG.\n- üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.\n\n## üôè Thanks to our sponsors\n\n<table align=\"center\" cellpadding=\"16\" cellspacing=\"12\">\n  <tr>\n    <td align=\"center\">\n      <a href=\"https://github.com/tinyfish-io/tinyfish-cookbook\" target=\"_blank\" rel=\"noopener\" title=\"TinyFish\">\n        <img src=\"docs/banner/sponsors/tinyfish.png\" alt=\"TinyFish\" width=\"500\">\n      </a>\n      <br>\n      <a href=\"https://github.com/tinyfish-io/tinyfish-cookbook\" target=\"_blank\" rel=\"noopener\" style=\"text-decoration: none; color: #333; font-weight: bold; font-size: 18px;\">\n        TinyFish\n      </a>\n    </td>\n    <td align=\"center\">\n      <a href=\"https://tsdb.co/shubham-gh\" target=\"_blank\" rel=\"noopener\" title=\"Tiger Data\">\n        <img src=\"docs/banner/sponsors/tigerdata.png\" alt=\"Tiger Data\" width=\"500\">\n      </a>\n      <br>\n      <a href=\"https://tsdb.co/shubham-gh\" target=\"_blank\" rel=\"noopener\" style=\"text-decoration: none; color: #333; font-weight: bold; font-size: 18px;\">\n        Tiger Data MCP\n      </a>\n    </td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n      <a href=\"https://github.com/speechmatics/speechmatics-academy\" target=\"_blank\" rel=\"noopener\" title=\"Speechmatics\">\n        <img src=\"docs/banner/sponsors/speechmatics.png\" alt=\"Speechmatics\" width=\"500\">\n      </a>\n      <br>\n      <a href=\"https://github.com/speechmatics/speechmatics-academy\" target=\"_blank\" rel=\"noopener\" style=\"text-decoration: none; color: #333; font-weight: bold; font-size: 18px;\">\n        Speechmatics\n      </a>\n    </td>\n    <td align=\"center\">\n      <a href=\"https://sponsorunwindai.com/\" title=\"Become a Sponsor\">\n        <img src=\"docs/banner/sponsor_awesome_llm_apps.png\" alt=\"Become a Sponsor\" width=\"500\">\n      </a>\n      <br>\n      <a href=\"https://sponsorunwindai.com/\" style=\"text-decoration: none; color: #333; font-weight: bold; font-size: 18px;\">\n        Become a Sponsor\n      </a>\n    </td>\n  </tr>\n</table>\n\n## üìÇ Featured AI Projects\n\n### AI Agents\n\n### üå± Starter AI Agents\n\n*   [üéôÔ∏è AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)\n*   [‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)\n*   [üìä AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)\n*   [ü©ª AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)\n*   [üòÇ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)\n*   [üéµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)\n*   [üõ´ AI Travel Agent (Local & Cloud)](starter_ai_agents/ai_travel_agent/)\n*   [‚ú® Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)\n*   [üîÑ Mixture of Agents](starter_ai_agents/mixture_of_agents/)\n*   [üìä xAI Finance Agent](starter_ai_agents/xai_finance_agent/)\n*   [üîç OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)\n*   [üï∏Ô∏è Web Scraping AI Agent (Local & Cloud SDK)](starter_ai_agents/web_scrapping_ai_agent/)\n\n### üöÄ Advanced AI Agents\n*   [üèöÔ∏è üçå AI Home Renovation Agent with Nano Banana Pro](advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent)\n*   [üîç AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)\n*   [üìä AI VC Due Diligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team)\n*   [üî¨ AI Research Planner & Executor (Google Interactions API)](advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api)\n*   [ü§ù AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)\n*   [üèóÔ∏è AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)\n*   [üí∞ AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)\n*   [üé¨ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)\n*   [üìà AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)\n*   [üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health & Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)\n*   [üöÄ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)\n*   [üóûÔ∏è AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)\n*   [üß† AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)\n*   [üìë AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)\n*   [üß¨ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)\n*   [üë®üèª‚Äçüíº AI Sales Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team)\n*   [üéß AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)\n*   [üåê Openwork - Open Browser Automation Agent](https://github.com/accomplish-ai/openwork)\n\n### üéÆ Autonomous Game Playing Agents\n\n*   [üéÆ AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)\n*   [‚ôú AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)\n*   [üé≤ AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)\n\n### ü§ù Multi-agent Teams\n\n*   [üß≤ AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)\n*   [üí≤ AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)\n*   [üé® AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)\n*   [üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud & Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)\n*   [üíº AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)\n*   [üè† AI Real Estate Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team)\n*   [üë®‚Äçüíº AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)\n*   [üë®‚Äçüè´ AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)\n*   [üíª Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)\n*   [‚ú® Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)\n*   [üé® üçå Multimodal UI/UX Feedback Agent Team with Nano Banana](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/)\n*   [üåè AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)\n\n### üó£Ô∏è Voice AI Agents\n\n*   [üó£Ô∏è AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)\n*   [üìû Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)\n*   [üîä Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)\n*   [üéôÔ∏è OpenSource Voice Dictation Agent (like Wispr Flow](https://github.com/akshayaggarwal99/jarvis-ai-assistant)\n\n### <img src=\"https://cdn.simpleicons.org/modelcontextprotocol\"  alt=\"mcp logo\" width=\"25\" height=\"20\"> MCP AI Agents \n\n*   [‚ôæÔ∏è Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)\n*   [üêô GitHub MCP Agent](mcp_ai_agents/github_mcp_agen"
  },
  {
    "full_name": "rowboatlabs/rowboat",
    "name": "rowboat",
    "description": "Open-source AI coworker, with memory",
    "language": "TypeScript",
    "today_stars": "191",
    "total_stars": "5370",
    "metadata": {
      "topics": [
        "agents",
        "agents-sdk",
        "ai",
        "ai-agents",
        "ai-agents-automation",
        "chatgpt",
        "claude-code",
        "claude-cowork",
        "generative-ai",
        "llm",
        "multiagent",
        "opeani",
        "open-source",
        "orchestration",
        "productivity"
      ],
      "license": "Apache-2.0",
      "forks": 430,
      "open_issues": 38,
      "created_at": "2025-01-13",
      "updated_at": "2026-02-13",
      "homepage": "https://www.rowboatlabs.com",
      "default_branch": "main",
      "size_kb": 76770,
      "watchers": 38,
      "archived": false
    },
    "readme_content": "<a href=\"https://www.youtube.com/watch?v=5AWoGo-L16I\" target=\"_blank\" rel=\"noopener noreferrer\">\n  <img width=\"1339\" height=\"607\" alt=\"rowboat-github-2\" src=\"https://github.com/user-attachments/assets/fc463b99-01b3-401c-b4a4-044dad480901\" />\n</a>\n\n<h5 align=\"center\">\n\n<p align=\"center\" style=\"display: flex; justify-content: center; gap: 20px; align-items: center;\">\n  <a href=\"https://trendshift.io/repositories/13609\" target=\"blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/13609\" alt=\"rowboatlabs/rowboat | Trendshift\" width=\"250\" height=\"55\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://www.rowboatlabs.com/\" target=\"_blank\" rel=\"noopener\">\n    <img alt=\"Website\" src=\"https://img.shields.io/badge/Website-10b981?labelColor=10b981&logo=window&logoColor=white\">\n  </a>\n  <a href=\"https://discord.gg/wajrgmJQ6b\" target=\"_blank\" rel=\"noopener\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-5865F2?logo=discord&logoColor=white&labelColor=5865F2\">\n  </a>\n  <a href=\"https://x.com/intent/user?screen_name=rowboatlabshq\" target=\"_blank\" rel=\"noopener\">\n    <img alt=\"Twitter\" src=\"https://img.shields.io/twitter/follow/rowboatlabshq?style=social\">\n  </a>\n  <a href=\"https://www.ycombinator.com\" target=\"_blank\" rel=\"noopener\">\n    <img alt=\"Y Combinator\" src=\"https://img.shields.io/badge/Y%20Combinator-S24-orange\">\n  </a>\n</p>\n\n# Rowboat  \n**Open-source AI coworker that turns work into a knowledge graph and acts on it**\n\n</h5>\n\nRowboat connects to your email and meeting notes, builds a long-lived knowledge graph, and uses that context to help you get work done - privately, on your machine.\n\nYou can do things like:\n- `Build me a deck about our next quarter roadmap` ‚Üí generates a PDF using context from your knowledge graph\n- `Prep me for my meeting with Alex` ‚Üí pulls past decisions, open questions, and relevant threads into a crisp brief (or a voice note)\n- Visualize, edit, and update your knowledge graph anytime (it‚Äôs just Markdown)\n- Record voice memos that automatically capture and update key takeaways in the graph\n\nDownload latest for Mac/Windows/Linux: [Download](https://www.rowboatlabs.com/downloads)\n\n\n## Demo\n\n\n[![Demo](https://github.com/user-attachments/assets/3f560bcf-d93c-4064-81eb-75a9fae31742)](https://www.youtube.com/watch?v=5AWoGo-L16I)\n\n[Watch the full video](https://www.youtube.com/watch?v=5AWoGo-L16I)\n\n---\n\n## Installation\n\n**Download latest for Mac/Windows/Linux:** [Download](https://www.rowboatlabs.com/downloads)\n\n**All release files:**   https://github.com/rowboatlabs/rowboat/releases/latest\n\n### Google setup\nTo connect Google services (Gmail, Calendar, and Drive), follow [Google setup](https://github.com/rowboatlabs/rowboat/blob/main/google-setup.md).\n\n### Voice notes\nTo enable voice notes (optional), add a Deepgram API key in ~/.rowboat/config/deepgram.json:\n```\n{\n  \"apiKey\": \"<key>\"\n}\n```\n\n\n## What it does\n\nRowboat is a **local-first AI coworker** that can:\n- **Remember** the important context you don‚Äôt want to re-explain (people, projects, decisions, commitments)\n- **Understand** what‚Äôs relevant right now (before a meeting, while replying to an email, when writing a doc)\n- **Help you act** by drafting, summarizing, planning, and producing real artifacts (briefs, emails, docs, PDF slides)\n\nUnder the hood, Rowboat maintains an **Obsidian-compatible vault** of plain Markdown notes with backlinks ‚Äî a transparent ‚Äúworking memory‚Äù you can inspect and edit.\n\n## Integrations\n\nRowboat builds memory from the work you already do, including:\n- **Gmail** (email)\n- **Granola** (meeting notes)\n- **Fireflies** (meeting notes)\n\n## How it‚Äôs different\n\nMost AI tools reconstruct context on demand by searching transcripts or documents.\n\nRowboat maintains **long-lived knowledge** instead:\n- context accumulates over time\n- relationships are explicit and inspectable\n- notes are editable by you, not hidden inside a model\n- everything lives on your machine as plain Markdown\n\nThe result is memory that compounds, rather than retrieval that starts cold every time.\n\n## What you can do with it\n\n- **Meeting prep** from prior decisions, threads, and open questions\n- **Email drafting** grounded in history and commitments\n- **Docs & decks** generated from your ongoing context (including PDF slides)\n- **Follow-ups**: capture decisions, action items, and owners so nothing gets dropped\n- **On-your-machine help**: create files, summarize into notes, and run workflows using local tools (with explicit, reviewable actions)\n\n## Background agents\n\nRowboat can spin up **background agents** to do repeatable work automatically - so routine tasks happen without you having to ask every time.\n\nExamples:\n- Draft email replies in the background (grounded in your past context and commitments)\n- Generate a daily voice note each morning (agenda, priorities, upcoming meetings)\n- Create recurring project updates from the latest emails/notes\n- Keep your knowledge graph up to date as new information comes in\n\nYou control what runs, when it runs, and what gets written back into your local Markdown vault.\n\n## Bring your own model\n\nRowboat works with the model setup you prefer:\n- **Local models** via Ollama or LM Studio\n- **Hosted models** (bring your own API key/provider)\n- Swap models anytime ‚Äî your data stays in your local Markdown vault\n\n## Extend Rowboat with tools (MCP)\n\nRowboat can connect to external tools and services via **Model Context Protocol (MCP)**.\nThat means you can plug in (for example) search, databases, CRMs, support tools, and automations - or your own internal tools.\n\nExamples: Exa (web search), Twitter/X, ElevenLabs (voice), Slack, Linear/Jira, GitHub, and more.\n\n## Local-first by design\n\n- All data is stored locally as plain Markdown\n- No proprietary formats or hosted lock-in\n- You can inspect, edit, back up, or delete everything at any time\n\n\n## Looking for Rowboat Web Studio?\n\nIf you‚Äôre looking for Rowboat web Studio, start [here](https://docs.rowboatlabs.com/). \n\n---\n<div align=\"center\">\n\n[Discord](https://discord.gg/wajrgmJQ6b) ¬∑ [Twitter](https://x.com/intent/user?screen_name=rowboatlabshq)\n</div>\n"
  },
  {
    "full_name": "github/gh-aw",
    "name": "gh-aw",
    "description": "GitHub Agentic Workflows",
    "language": "Go",
    "today_stars": "405",
    "total_stars": "2097",
    "metadata": {
      "topics": [
        "actions",
        "cai",
        "ci",
        "claude-code",
        "codex",
        "copilot",
        "gh-extension",
        "github-actions"
      ],
      "license": "MIT",
      "forks": 148,
      "open_issues": 44,
      "created_at": "2025-08-12",
      "updated_at": "2026-02-13",
      "homepage": "https://gh.io/gh-aw",
      "default_branch": "main",
      "size_kb": 768175,
      "watchers": 6,
      "archived": false
    },
    "readme_content": "<!--\n\nHello fellow agent! Welcome to GitHub Agentic Workflows = Actions + Agent + Safety. \nHere are some pointers to get you started in using this tool.\n\n- Create a new workflow: https://raw.githubusercontent.com/github/gh-aw/main/create.md\n- Install: https://raw.githubusercontent.com/github/gh-aw/main/install.md\n- Reference: https://raw.githubusercontent.com/github/gh-aw/main/.github/aw/github-agentic-workflows.md\n\n-->\n\n# GitHub Agentic Workflows\n\nWrite agentic workflows in natural language markdown, and run them in GitHub Actions.\n\n## Contents\n\n- [Quick Start](#quick-start)\n- [Overview](#overview)\n- [Guardrails](#guardrails)\n- [Documentation](#documentation)\n- [Contributing](#contributing)\n- [Share Feedback](#share-feedback)\n- [Peli's Agent Factory](#pelis-agent-factory)\n- [Related Projects](#related-projects)\n\n## Quick Start\n\nReady to get your first agentic workflow running? Follow our step-by-step [Quick Start Guide](https://github.github.com/gh-aw/setup/quick-start/) to install the extension, add a sample workflow, and see it in action.\n\n## Overview\n\nLearn about the concepts behind agentic workflows, explore available workflow types, and understand how AI can automate your repository tasks. See [How It Works](https://github.github.io/gh-aw/introduction/how-they-work/).\n\n## Guardrails\n\nGuardrails, safety and security are foundational to GitHub Agentic Workflows. Workflows run with read-only permissions by default, with write operations only allowed through sanitized `safe-outputs`. The system implements multiple layers of protection including sandboxed execution, input sanitization, network isolation, supply chain security (SHA-pinned dependencies), tool allow-listing, and compile-time validation. Access can be gated to team members only, with human approval gates for critical operations, ensuring AI agents operate safely within controlled boundaries. See the [Security Architecture](https://github.github.com/gh-aw/introduction/architecture/) for comprehensive details on threat modeling, implementation guidelines, and best practices.\n\nUsing agentic workflows in your repository requires careful attention to security considerations and careful human supervision, and even then things can still go wrong. Use it with caution, and at your own risk.\n\n## Documentation\n\nFor complete documentation, examples, and guides, see the [Documentation](https://github.github.com/gh-aw/).\n\n## Contributing\n\nFor development setup and contribution guidelines, see [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Share Feedback\n\nWe welcome your feedback on GitHub Agentic Workflows! Please file bugs and feature requests as issues in this repository, and share your thoughts in the [GitHub Next Discord](https://gh.io/next-discord).\n\n## Peli's Agent Factory\n\nSee the [Peli's Agent Factory](https://github.github.com/gh-aw/blog/2026-01-12-welcome-to-pelis-agent-factory/) for a guided tour through many uses of agentic workflows.\n\n## Related Projects\n\nGitHub Agentic Workflows is supported by companion projects that provide additional security and integration capabilities:\n\n- **[Agent Workflow Firewall (AWF)](https://github.com/github/gh-aw-firewall)** - Network egress control for AI agents, providing domain-based access controls and activity logging for secure workflow execution\n- **[MCP Gateway](https://github.com/github/gh-aw-mcpg)** - Routes Model Context Protocol (MCP) server calls through a unified HTTP gateway for centralized access management\n",
    "manifest_content": "module github.com/github/gh-aw\n\ngo 1.25.0\n\nrequire (\n\tgithub.com/charmbracelet/bubbles v0.21.1\n\tgithub.com/charmbracelet/bubbletea v1.3.10\n\tgithub.com/charmbracelet/huh v0.8.0\n\tgithub.com/charmbracelet/lipgloss v1.1.1-0.20250319133953-166f707985bc\n\tgithub.com/charmbracelet/x/exp/golden v0.0.0-20251215102626-e0db08df7383\n\tgithub.com/cli/go-gh/v2 v2.13.0\n\tgithub.com/creack/pty v1.1.24\n\tgithub.com/fsnotify/fsnotify v1.9.0\n\tgithub.com/goccy/go-yaml v1.19.2\n\tgithub.com/google/jsonschema-go v0.4.2\n\tgithub.com/modelcontextprotocol/go-sdk v1.3.0\n\tgithub.com/rhysd/actionlint v1.7.10\n\tgithub.com/santhosh-tekuri/jsonschema/v6 v6.0.2\n\tgithub.com/securego/gosec/v2 v2.22.11\n\tgithub.com/sourcegraph/conc v0.3.0\n\tgithub.com/spf13/cobra v1.10.2\n\tgithub.com/stretchr/testify v1.11.1\n\tgolang.org/x/crypto v0.48.0\n\tgolang.org/x/mod v0.33.0\n\tgolang.org/x/term v0.40.0\n\tgolang.org/x/tools/gopls v0.21.0\n\tgolang.org/x/vuln v1.1.4\n)\n\nrequire (\n\tcloud.google.com/go v0.121.2 // indirect\n\tcloud.google.com/go/auth v0.16.5 // indirect\n\tcloud.google.com/go/compute/metadata v0.8.0 // indirect\n\tgithub.com/BurntSushi/toml v1.6.0 // indirect\n\tgithub.com/anthropics/anthropic-sdk-go v1.19.0 // indirect\n\tgithub.com/atotto/clipboard v0.1.4 // indirect\n\tgithub.com/aymanbagabas/go-osc52/v2 v2.0.1 // indirect\n\tgithub.com/aymanbagabas/go-udiff v0.3.1 // indirect\n\tgithub.com/bmatcuk/doublestar/v4 v4.9.1 // indirect\n\tgithub.com/catppuccin/go v0.3.0 // indirect\n\tgithub.com/ccojocar/zxcvbn-go v1.0.4 // indirect\n\tgithub.com/charmbracelet/colorprofile v0.4.1 // indirect\n\tgithub.com/charmbracelet/harmonica v0.2.0 // indirect\n\tgithub.com/charmbracelet/x/ansi v0.11.5 // indirect\n\tgithub.com/charmbracelet/x/cellbuf v0.0.15 // indirect\n\tgithub.com/charmbracelet/x/exp/strings v0.0.0-20251106172358-54469c29c2bc // indirect\n\tgithub.com/charmbracelet/x/term v0.2.2 // indirect\n\tgithub.com/cli/safeexec v1.0.1 // indirect\n\tgithub.com/cli/shurcooL-graphql v0.0.4 // indirect\n\tgithub.com/clipperhouse/displaywidth v0.9.0 // indirect\n\tgithub.com/clipperhouse/stringish v0.1.1 // indirect\n\tgithub.com/clipperhouse/uax29/v2 v2.5.0 // indirect\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/dlclark/regexp2 v1.11.5 // indirect\n\tgithub.com/dustin/go-humanize v1.0.1 // indirect\n\tgithub.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f // indirect\n\tgithub.com/fatih/camelcase v1.0.0 // indirect\n\tgithub.com/fatih/color v1.18.0 // indirect\n\tgithub.com/fatih/gomodifytags v1.17.1-0.20250423142747-f3939df9aa3c // indirect\n\tgithub.com/fatih/structtag v1.2.0 // indirect\n\tgithub.com/felixge/httpsnoop v1.0.4 // indirect\n\tgithub.com/go-logr/logr v1.4.3 // indirect\n\tgithub.com/go-logr/stdr v1.2.2 // indirect\n\tgithub.com/google/go-cmp v0.7.0 // indirect\n\tgithub.com/google/s2a-go v0.1.9 // indirect\n\tgithub.com/google/uuid v1.6.0 // indirect\n\tgithub.com/googleapis/enterprise-certificate-proxy v0.3.6 // indirect\n\tgithub.com/googleapis/gax-go/v2 v2.15.0 // indirect\n\tgithub.com/gookit/color v1.6.0 // indirect\n\tgithub.com/gorilla/websocket v1.5.3 // indirect\n\tgithub.com/henvic/httpretty v0.1.4 // indirect\n\tgithub.com/inconshreveable/mousetrap v1.1.0 // indirect\n\tgithub.com/lucasb-eyer/go-colorful v1.3.0 // indirect\n\tgithub.com/mattn/go-colorable v0.1.14 // indirect\n\tgithub.com/mattn/go-isatty v0.0.20 // indirect\n\tgithub.com/mattn/go-localereader v0.0.1 // indirect\n\tgithub.com/mattn/go-runewidth v0.0.19 // indirect\n\tgithub.com/mattn/go-shellwords v1.0.12 // indirect\n\tgithub.com/mitchellh/hashstructure/v2 v2.0.2 // indirect\n\tgithub.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 // indirect\n\tgithub.com/muesli/cancelreader v0.2.2 // indirect\n\tgithub.com/muesli/termenv v0.16.0 // indirect\n\tgithub.com/openai/openai-go/v3 v3.8.1 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgithub.com/rivo/uniseg v0.4.7 // indirect\n\tgithub.com/robfig/cron/v3 v3.0.1 // indirect\n\tgithub.com/sahilm/fuzzy v0.1.1 // indirect\n\tgithub.com/spf13/pflag v1.0.10 // indirect\n\tgithub.com/thlib/go-timezone-local v0.0.7 // indirect\n\tgithub.com/tidwall/gjson v1.18.0 // indirect\n\tgithub.com/tidwall/match v1.1.1 // indirect\n\tgithub.com/tidwall/pretty v1.2.1 // indirect\n\tgithub.com/tidwall/sjson v1.2.5 // indirect\n\tgithub.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e // indirect\n\tgithub.com/yosida95/uritemplate/v3 v3.0.2 // indirect\n\tgo.opentelemetry.io/auto/sdk v1.1.0 // indirect\n\tgo.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.61.0 // indirect\n\tgo.opentelemetry.io/otel v1.37.0 // indirect\n\tgo.opentelemetry.io/otel/metric v1.37.0 // indirect\n\tgo.opentelemetry.io/otel/trace v1.37.0 // indirect\n\tgo.uber.org/multierr v1.11.0 // indirect\n\tgo.yaml.in/yaml/v3 v3.0.4 // indirect\n\tgo.yaml.in/yaml/v4 v4.0.0-rc.3 // indirect\n\tgolang.org/x/exp v0.0.0-20240909161429-701f63a606c0 // indirect\n\tgolang.org/x/exp/typeparams v0.0.0-20251023183803-a4bb9ffd2546 // indirect\n\tgolang.org/x/net v0.49.0 // indirect\n\tgolang.org/x/oauth2 v0.34.0 // indirect\n\tgolang.org/x/sync v0.19.0 // indirect\n\tgolang.org/x"
  },
  {
    "full_name": "unslothai/unsloth",
    "name": "unsloth",
    "description": "Fine-tuning & Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek, Qwen, Llama, Gemma, TTS 2x faster with 70% less VRAM.",
    "language": "Python",
    "today_stars": "81",
    "total_stars": "52145",
    "metadata": {
      "topics": [
        "agent",
        "deepseek",
        "deepseek-r1",
        "fine-tuning",
        "gemma",
        "gemma3",
        "gpt-oss",
        "llama",
        "llama3",
        "llm",
        "llms",
        "mistral",
        "openai",
        "qwen",
        "qwen3",
        "reinforcement-learning",
        "text-to-speech",
        "tts",
        "unsloth",
        "voice-cloning"
      ],
      "license": "Apache-2.0",
      "forks": 4315,
      "open_issues": 954,
      "created_at": "2023-11-29",
      "updated_at": "2026-02-13",
      "homepage": "https://unsloth.ai/docs",
      "default_branch": "main",
      "size_kb": 9469,
      "watchers": 299,
      "archived": false
    },
    "readme_content": "<div align=\"center\">\n\n  <a href=\"https://unsloth.ai/docs\"><picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\">\n    <img alt=\"unsloth logo\" src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" height=\"110\" style=\"max-width: 100%;\">\n  </picture></a>\n  \n<a href=\"https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png\" width=\"154\"></a>\n<a href=\"https://discord.com/invite/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" width=\"165\"></a>\n<a href=\"https://unsloth.ai/docs\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png\" width=\"137\"></a>\n\n### Train gpt-oss, DeepSeek, Gemma, Qwen & Llama 2x faster with 70% less VRAM!\n\n![](https://i.ibb.co/sJ7RhGG/image-41.png)\n\n</div>\n\n## ‚ú® Train for Free\n\nNotebooks are beginner friendly. Read our [guide](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide). Add dataset, run, then deploy your trained model.\n\n| Model | Free Notebooks | Performance | Memory use |\n|-----------|---------|--------|----------|\n| **gpt-oss (20B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb)               | 1.5x faster | 70% less |\n| **gpt-oss (20B): GRPO**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb)               | 2x faster | 80% less |\n| **Qwen3: Advanced GRPO**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)               | 2x faster | 50% less |\n| **Qwen3-VL (8B): GSPO**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_VL_(8B)-Vision-GRPO.ipynb)               | 1.5x faster | 80% less |\n| **Gemma 3 (4B) Vision** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B)-Vision.ipynb)               | 1.7x faster | 60% less |\n| **Gemma 3n (e4B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb)               | 1.5x faster | 50% less |\n| **embeddinggemma (300M)**    | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/EmbeddingGemma_(300M).ipynb)               | 2x faster | 20% less |\n| **Mistral Ministral 3 (3B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Ministral_3_VL_(3B)_Vision.ipynb)               | 1.5x faster | 60% less |\n| **Llama 3.1 (8B) Alpaca**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2x faster | 70% less |\n| **Llama 3.2 Conversational**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2x faster | 70% less |\n| **Orpheus-TTS (3B)**     | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb)               | 1.5x faster | 50% less |\n\n- See all our notebooks for: [Kaggle](https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks), [GRPO](https://unsloth.ai/docs/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks), [TTS](https://unsloth.ai/docs/get-started/unsloth-notebooks#text-to-speech-tts-notebooks), [embedding](https://unsloth.ai/docs/new/embedding-finetuning) & [Vision](https://unsloth.ai/docs/get-started/unsloth-notebooks#vision-multimodal-notebooks)\n- See [all our models](https://unsloth.ai/docs/get-started/unsloth-model-catalog) and [all our notebooks](https://unsloth.ai/docs/get-started/unsloth-notebooks)\n- See detailed documentation for Unsloth [here](https://unsloth.ai/docs)\n\n## ‚ö° Quickstart\n### Linux or WSL\n```bash\npip install unsloth\n```\n### Windows\nFor Windows, `pip install unsloth` works only if you have Pytorch installed. Read our [Windows Guide](https://unsloth.ai/docs/get-started/install/windows-installation).\n\n### Docker\nUse our official [Unsloth Docker image](https://hub.docker.com/r/unsloth/unsloth) ```unsloth/unsloth``` container. Read our [Docker Guide](https://unsloth.ai/docs/get-started/install/docker).\n\n### Blackwell & DGX Spark\nFor RTX 50x, B200, 6000 GPUs: `pip install unsloth`. Read our [Blackwell Guide](https://unsloth.ai/docs/blog/fine-tuning-llms-with-blackwell-rtx-50-series-and-unsloth) and [DGX Spark Guide](https://unsloth.ai/docs/blog/fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth) for more details.\n\n## ü¶• Unsloth News\n- **Embedding models**: Unsloth now supports ~1.8-3.3x faster embedding fine-tuning. [Blog](https://unsloth.ai/docs/new/embedding-finetuning) ‚Ä¢ [Notebooks](https://unsloth.ai/docs/get-started/unsloth-notebooks#embedding-models)\n- New **7x longer context RL** vs. all other setups, via our new batching algorithms. [Blog](https://unsloth.ai/docs/new/grpo-long-context)\n- New RoPE & MLP **Triton Kernels** & **Padding Free + Packing**: 3x faster training & 30% less VRAM. [Blog](https://unsloth.ai/docs/new/3x-faster-training-packing)\n- **500K Context**: Training a 20B model with >500K context is now possible on an 80GB GPU. [Blog](https://unsloth.ai/docs/blog/500k-context-length-fine-tuning)\n- **FP8 Reinforcement Learning**: You can now do FP8 GRPO on consumer GPUs. [Blog](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning) ‚Ä¢ [Notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_8B_FP8_GRPO.ipynb)\n- **DeepSeek-OCR**: Fine-tune to improve language understanding by 89%. [Guide](https://unsloth.ai/docs/models/tutorials/deepseek-ocr-how-to-run-and-fine-tune) ‚Ä¢ [Notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Deepseek_OCR_(3B).ipynb)\n- **Docker**: Use Unsloth with no setup & environment issues with our new image. [Guide](https://unsloth.ai/docs/blog/how-to-fine-tune-llms-with-unsloth-and-docker) ‚Ä¢ [Docker image](https://hub.docker.com/r/unsloth/unsloth)\n- **Vision RL**: You can now train VLMs with GRPO or GSPO in Unsloth! [Read guide](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/vision-reinforcement-learning-vlm-rl)\n- **gpt-oss** by OpenAI: Read our [RL blog](https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune/gpt-oss-reinforcement-learning), [Flex Attention](https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune/long-context-gpt-oss-training) blog and [gpt-oss Guide](https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune). 20B works on 14GB VRAM. 120B on 65GB.\n\n<details>\n  <summary>Click for more news</summary>\n\n- **Quantization-Aware Training**: We collabed with Pytorch, recovering ~70% accuracy. [Read blog](https://unsloth.ai/docs/blog/quantization-aware-training-qat)\n- **Memory-efficient RL**: We're introducing even better RL. Our new kernels & algos allows faster RL with 50% less VRAM & 10√ó more context. [Read blog](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/memory-efficient-rl)\n- **Mistral 3**: Run Ministral 3 or Devstral 2 and fine-tune with vision/RL sudoku notebooks. [Guide](https://unsloth.ai/docs/models/tutorials/ministral-3) ‚Ä¢ [Notebooks](https://unsloth.ai/docs/models/ministral-3#fine-tuning-ministral-3)\n- **Gemma 3n** by Google: [Read Blog](https://unsloth.ai/docs/models/gemma-3-how-to-run-and-fine-tune/gemma-3n-how-to-run-and-fine-tune). We [uploaded GGUFs, 4-bit models](https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339).\n- **[Text-to-Speech (TTS)](https://unsloth.ai/docs/basics/text-to-speech-tts-fine-tuning)** is now supported, including `sesame/csm-1b` and STT `openai/whisper-large-v3`.\n- **[Qwen3](https://unsloth.ai/docs/models/qwen3-how-to-run-and-fine-tune)** is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.\n- Introducing **[Dynamic 2.0](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs)** quants that set new benchmarks on 5-shot MMLU & Aider Polyglot.\n- [**EVERYTHING** is now supported](https://unsloth.ai/blog/gemma3#everything) - all models (TTS, BERT, Mamba), FFT, etc. [MultiGPU](https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth) is now supported. Enable FFT with `full_finetuning = True`, 8-bit with `load_in_8bit = True`.\n- üì£ [DeepSeek-R1](https://unsloth.ai/blog/deepseek-r1) - run or fine-tune them [with our guide](https://unsloth.ai/blog/deepseek-r1). All model uploads: [here](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5).\n- üì£ Introducing Long-context [Reasoning (GRPO)](https://unsloth.ai/blog/grpo) in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!\n- üì£ Introducing Unsloth [Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using <10% more VRAM than BnB 4-bit. See our collection on [Hugging Face here.](https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7)\n- üì£ **[Llama 4](https://unsloth.ai/blog/llama4)** by Meta, including Scout & Maverick are now supported.\n- üì£ [Phi-4](https://unsloth.ai/blog/phi4) by Microsoft: We also [fixed bugs](https://unsloth.ai/blog/phi4) in Phi-4 and [uploaded GGUFs, 4-bit",
    "manifest_content": "[build-system]\nrequires = [\"setuptools==80.9.0\", \"setuptools-scm==9.2.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"unsloth\"\ndynamic = [\"version\"]\ndescription = \"2-5X faster training, reinforcement learning & finetuning\"\nreadme = \"README.md\"\nrequires-python = \">=3.9,<3.14\"\nlicense = \"Apache-2.0\"\nkeywords = [\"ai\", \"llm\", \"reinforcement learning\", \"machine learning\", \"artificial intelligence\", \"pytorch\"]\nauthors = [\n    {email = \"info@unsloth.ai\"},\n    {name = \"Unsloth AI team\"},\n]\nmaintainers = [\n    {name = \"Daniel Han\", email = \"danielhanchen@gmail.com\"},\n    {name = \"Michael Han\", email = \"info@unsloth.ai\"},\n]\nclassifiers = [\n    \"Programming Language :: Python\",\n    \"Environment :: GPU\",\n    \"Environment :: GPU :: NVIDIA CUDA\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n]\n\n[tool.setuptools.dynamic]\nversion = {attr = \"unsloth.models._utils.__version__\"}\n\n[tool.setuptools]\ninclude-package-data = false\n\n[tool.setuptools.packages.find]\nexclude = [\"images*\", \"tests*\", \"kernels/moe*\"]\n\n[project.optional-dependencies]\ntriton = [\n    \"triton>=3.0.0 ; ('linux' in sys_platform)\",\n    \"triton-windows ; (sys_platform == 'win32') and (platform_machine == 'AMD64' or platform_machine == 'x86_64')\",\n]\n\nhuggingfacenotorch = [\n    \"wheel>=0.42.0\",\n    \"packaging\",\n    \"numpy\",\n    \"tqdm\",\n    \"psutil\",\n    \"tyro\",\n    \"protobuf\",\n    \"sentencepiece>=0.2.0\",\n    \"datasets>=3.4.1,!=4.0.*,!=4.1.0,<4.4.0\",\n    \"accelerate>=0.34.1\",\n    \"peft>=0.18.0,!=0.11.0\",\n    \"huggingface_hub>=0.34.0\",\n    \"hf_transfer\",\n    \"diffusers\",\n    \"transformers>=4.51.3,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6\",\n    \"trl>=0.18.2,!=0.19.0,<=0.24.0\",\n    \"sentence-transformers\",\n]\nhuggingface = [\n    \"unsloth[huggingfacenotorch]\",\n    \"unsloth_zoo>=2026.2.1\",\n    \"torchvision\",\n    \"unsloth[triton]\",\n]\nwindows = [\n    \"unsloth[huggingface]\",\n    \"bitsandbytes>=0.45.5,!=0.46.0,!=0.48.0 ; (sys_platform == 'win32')\",\n    \"xformers>=0.0.22.post7 ; (sys_platform == 'win32')\",\n]\nbase = [\n    \"unsloth[huggingface]\",\n]\ncu118only = [\n    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and ('linux' in sys_platform)\",\n    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and ('linux' in sys_platform)\",\n    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and ('linux' in sys_platform)\",\n]\ncu121only = [\n    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and ('linux' in sys_platform)\",\n    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and ('linux' in sys_platform)\",\n    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and ('linux' in sys_platform)\",\n]\ncu118onlytorch211 = [\n    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and ('linux' in sys_platform)\",\n    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and ('linux' in sys_platform)\",\n    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and ('linux' in sys_platform)\",\n]\ncu121onlytorch211 = [\n    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and ('linux' in sys_platform)\",\n    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and ('linux' in sys_platform)\",\n    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and ('linux' in sys_platform)\",\n]\ncu118onlytorch212 = [\n    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and ('linux' in sys_platform)\",\n    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and ('linux' in sys_platform)\",\n    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and ('linux' in sys_platform)\",\n]\ncu121onlytorch212 = [\n    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and ('linux' in sys_platform)\",\n    \"x"
  },
  {
    "full_name": "cinnyapp/cinny",
    "name": "cinny",
    "description": "Yet another matrix client",
    "language": "TypeScript",
    "today_stars": "38",
    "total_stars": "2936",
    "metadata": {
      "topics": [
        "cinny",
        "cinnyapp",
        "client",
        "hacktoberfest",
        "matrix",
        "matrix-client",
        "matrix-org",
        "reactjs"
      ],
      "license": "AGPL-3.0",
      "forks": 396,
      "open_issues": 320,
      "created_at": "2021-04-18",
      "updated_at": "2026-02-13",
      "homepage": "https://cinny.in",
      "default_branch": "dev",
      "size_kb": 11673,
      "watchers": 19,
      "archived": false
    },
    "readme_content": "# Cinny\n<p>\n    <a href=\"https://github.com/ajbura/cinny/releases\">\n        <img alt=\"GitHub release downloads\" src=\"https://img.shields.io/github/downloads/ajbura/cinny/total?logo=github&style=social\"></a>\n    <a href=\"https://hub.docker.com/r/ajbura/cinny\">\n        <img alt=\"DockerHub downloads\" src=\"https://img.shields.io/docker/pulls/ajbura/cinny?logo=docker&style=social\"></a>\n    <a href=\"https://fosstodon.org/@cinnyapp\">\n        <img alt=\"Follow on Mastodon\" src=\"https://img.shields.io/mastodon/follow/106845779685925461?domain=https%3A%2F%2Ffosstodon.org&logo=mastodon&style=social\"></a>\n    <a href=\"https://twitter.com/intent/follow?screen_name=cinnyapp\">\n        <img alt=\"Follow on Twitter\" src=\"https://img.shields.io/twitter/follow/cinnyapp?logo=twitter&style=social\"></a>\n    <a href=\"https://cinny.in/#sponsor\">\n        <img alt=\"Sponsor Cinny\" src=\"https://img.shields.io/opencollective/all/cinny?logo=opencollective&style=social\"></a>\n</p>\n\nA Matrix client focusing primarily on simple, elegant and secure interface. The main goal is to have an instant messaging application that is easy on people and has a modern touch.\n- [Roadmap](https://github.com/orgs/cinnyapp/projects/1)\n- [Contributing](./CONTRIBUTING.md)\n\n<img align=\"center\" src=\"https://raw.githubusercontent.com/cinnyapp/cinny-site/main/assets/preview2-light.png\" height=\"380\">\n\n## Getting started\nThe web app is available at [app.cinny.in](https://app.cinny.in/) and gets updated on each new release. The `dev` branch is continuously deployed at [dev.cinny.in](https://dev.cinny.in) but keep in mind that it could have things broken.\n\nYou can also download our desktop app from the [cinny-desktop repository](https://github.com/cinnyapp/cinny-desktop).\n\n## Self-hosting\nTo host Cinny on your own, simply download the tarball from [GitHub releases](https://github.com/cinnyapp/cinny/releases/latest), and serve the files from `dist/` using your preferred webserver. Alternatively, you can just pull the docker image from [DockerHub](https://hub.docker.com/r/ajbura/cinny) or [GitHub Container Registry](https://github.com/cinnyapp/cinny/pkgs/container/cinny).\n\n* The default homeservers and explore pages are defined in [`config.json`](config.json).\n\n* You need to set up redirects to serve the assests. Example configurations; [netlify](netlify.toml), [nginx](contrib/nginx/cinny.domain.tld.conf), [caddy](contrib/caddy/caddyfile).\n    * If you have trouble configuring redirects you can [enable hash routing](config.json#L35) ‚Äî the url in the browser will have a `/#/` between the domain and open channel (ie. `app.cinny.in/#/home/` instead of `app.cinny.in/home/`) but you won't have to configure your webserver.\n\n* To deploy on subdirectory, you need to rebuild the app youself after updating the `base` path in [`build.config.ts`](build.config.ts).\n    * For example, if you want to deploy on `https://cinny.in/app`, then set `base: '/app'`.\n\n<details><summary><b>PGP Public Key to verify tarball</b></summary>\n\n```\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\nmxFo+ioe/ABCufSmyqFye0psX3Sp\n=WtqZ\n-----END PGP PUBLIC KEY BLOCK-----\n```\n</details>\n\n## Local development\n> [!TIP]\n> We recommend using a version manager as versions change very quickly. You will likely need to switch between multiple Node.js versions based on the needs of different projects you're working on. [NVM on windows](https://github.com/coreybutler/nvm-windows#installation--upgrades) on Windows and [nvm](https://github.com/nvm-sh/nvm) on Linux/macOS are pretty good choices. Recommended nodejs version is Iron LTS (v20).\n\nExecute the following commands to start a development server:\n```sh\nnpm ci # Installs all dependencies\nnpm start # Serve a development version\n```\n\nTo build the app:\n```sh\nnpm run build # Compiles the app into the dist/ directory\n```\n\n### Running with Docker\nThis repository includes a Dockerfile, which builds the application from source and serves it with Nginx on port 80. To\nuse this locally, you can build the container like so:\n```\ndocker build -t cinny:latest .\n```\n\nYou can then run the container you've built with a command similar to this:\n```\ndocker run -p 8080:80 cinny:latest\n```\n\nThis will forward your `localhost` port 8080 to the container's port 80. You can visit the app in your browser by navigating to `http://localhost:8080`.\n",
    "manifest_content": "{\n  \"name\": \"cinny\",\n  \"version\": \"4.10.2\",\n  \"description\": \"Yet another matrix client\",\n  \"main\": \"index.js\",\n  \"type\": \"module\",\n  \"engines\": {\n    \"node\": \">=16.0.0\"\n  },\n  \"scripts\": {\n    \"start\": \"vite\",\n    \"build\": \"vite build\",\n    \"lint\": \"yarn check:eslint && yarn check:prettier\",\n    \"check:eslint\": \"eslint src/*\",\n    \"check:prettier\": \"prettier --check .\",\n    \"fix:prettier\": \"prettier --write .\",\n    \"typecheck\": \"tsc --noEmit\"\n  },\n  \"keywords\": [],\n  \"author\": \"Ajay Bura\",\n  \"license\": \"AGPL-3.0-only\",\n  \"dependencies\": {\n    \"@atlaskit/pragmatic-drag-and-drop\": \"1.1.6\",\n    \"@atlaskit/pragmatic-drag-and-drop-auto-scroll\": \"1.3.0\",\n    \"@atlaskit/pragmatic-drag-and-drop-hitbox\": \"1.0.3\",\n    \"@fontsource/inter\": \"4.5.14\",\n    \"@tanstack/react-query\": \"5.24.1\",\n    \"@tanstack/react-query-devtools\": \"5.24.1\",\n    \"@tanstack/react-virtual\": \"3.2.0\",\n    \"@vanilla-extract/css\": \"1.9.3\",\n    \"@vanilla-extract/recipes\": \"0.3.0\",\n    \"@vanilla-extract/vite-plugin\": \"3.7.1\",\n    \"await-to-js\": \"3.0.0\",\n    \"badwords-list\": \"2.0.1-4\",\n    \"blurhash\": \"2.0.4\",\n    \"browser-encrypt-attachment\": \"0.3.0\",\n    \"chroma-js\": \"3.1.2\",\n    \"classnames\": \"2.3.2\",\n    \"dateformat\": \"5.0.3\",\n    \"dayjs\": \"1.11.10\",\n    \"domhandler\": \"5.0.3\",\n    \"emojibase\": \"15.3.1\",\n    \"emojibase-data\": \"15.3.2\",\n    \"file-saver\": \"2.0.5\",\n    \"focus-trap-react\": \"10.0.2\",\n    \"folds\": \"2.5.0\",\n    \"html-dom-parser\": \"4.0.0\",\n    \"html-react-parser\": \"4.2.0\",\n    \"i18next\": \"23.12.2\",\n    \"i18next-browser-languagedetector\": \"8.0.0\",\n    \"i18next-http-backend\": \"2.5.2\",\n    \"immer\": \"9.0.16\",\n    \"is-hotkey\": \"0.2.0\",\n    \"jotai\": \"2.6.0\",\n    \"linkify-react\": \"4.1.3\",\n    \"linkifyjs\": \"4.1.3\",\n    \"matrix-js-sdk\": \"38.2.0\",\n    \"millify\": \"6.1.0\",\n    \"pdfjs-dist\": \"4.2.67\",\n    \"prismjs\": \"1.30.0\",\n    \"react\": \"18.2.0\",\n    \"react-aria\": \"3.29.1\",\n    \"react-blurhash\": \"0.2.0\",\n    \"react-colorful\": \"5.6.1\",\n    \"react-dom\": \"18.2.0\",\n    \"react-error-boundary\": \"4.0.13\",\n    \"react-google-recaptcha\": \"2.1.0\",\n    \"react-i18next\": \"15.0.0\",\n    \"react-range\": \"1.8.14\",\n    \"react-router-dom\": \"6.20.0\",\n    \"sanitize-html\": \"2.12.1\",\n    \"slate\": \"0.112.0\",\n    \"slate-dom\": \"0.112.2\",\n    \"slate-history\": \"0.110.3\",\n    \"slate-react\": \"0.112.1\",\n    \"ua-parser-js\": \"1.0.35\"\n  },\n  \"devDependencies\": {\n    \"@esbuild-plugins/node-globals-polyfill\": \"0.2.3\",\n    \"@rollup/plugin-inject\": \"5.0.3\",\n    \"@rollup/plugin-wasm\": \"6.1.1\",\n    \"@types/chroma-js\": \"3.1.1\",\n    \"@types/file-saver\": \"2.0.5\",\n    \"@types/is-hotkey\": \"0.1.10\",\n    \"@types/node\": \"18.11.18\",\n    \"@types/prismjs\": \"1.26.0\",\n    \"@types/react\": \"18.2.39\",\n    \"@types/react-dom\": \"18.2.17\",\n    \"@types/react-google-recaptcha\": \"2.1.8\",\n    \"@types/sanitize-html\": \"2.9.0\",\n    \"@types/ua-parser-js\": \"0.7.36\",\n    \"@typescript-eslint/eslint-plugin\": \"5.46.1\",\n    \"@typescript-eslint/parser\": \"5.46.1\",\n    \"@vitejs/plugin-react\": \"4.2.0\",\n    \"buffer\": \"6.0.3\",\n    \"eslint\": \"8.29.0\",\n    \"eslint-config-airbnb\": \"19.0.4\",\n    \"eslint-config-prettier\": \"8.5.0\",\n    \"eslint-plugin-import\": \"2.29.1\",\n    \"eslint-plugin-jsx-a11y\": \"6.6.1\",\n    \"eslint-plugin-react\": \"7.31.11\",\n    \"eslint-plugin-react-hooks\": \"4.6.0\",\n    \"prettier\": \"2.8.1\",\n    \"typescript\": \"4.9.4\",\n    \"vite\": \"5.4.19\",\n    \"vite-plugin-pwa\": \"0.20.5\",\n    \"vite-plugin-static-copy\": \"1.0.4\",\n    \"vite-plugin-top-level-await\": \"1.4.4\"\n  }\n}\n"
  },
  {
    "full_name": "Jeffallan/claude-skills",
    "name": "claude-skills",
    "description": "66 Specialized Skills for Full-Stack Developers. Transform Claude Code into your expert pair programmer.",
    "language": "Python",
    "today_stars": "278",
    "total_stars": "1968",
    "metadata": {
      "topics": [
        "ai-agents",
        "claude",
        "claude-code",
        "claude-marketplace",
        "claude-skills"
      ],
      "license": "MIT",
      "forks": 132,
      "open_issues": 29,
      "created_at": "2025-10-20",
      "updated_at": "2026-02-13",
      "homepage": "",
      "default_branch": "main",
      "size_kb": 8854,
      "watchers": 13,
      "archived": false
    },
    "readme_content": "<p align=\"center\">\n  <img src=\"https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=12,14,25,27&height=200&section=header&text=Claude%20Skills&fontSize=80&fontColor=ffffff&animation=fadeIn&fontAlignY=35&desc=66%20Skills%20%E2%80%A2%209%20Workflows%20%E2%80%A2%20Built%20for%20Full-Stack%20Devs&descSize=20&descAlignY=55\" width=\"100%\"/>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/jeffallan/claude-skills\"><img src=\"https://img.shields.io/badge/version-0.4.7-blue.svg?style=for-the-badge\" alt=\"Version\"/></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-green.svg?style=for-the-badge\" alt=\"License\"/></a>\n  <a href=\"https://github.com/jeffallan/claude-skills\"><img src=\"https://img.shields.io/badge/Claude_Code-Plugin-purple.svg?style=for-the-badge\" alt=\"Claude Code\"/></a>\n  <a href=\"https://github.com/jeffallan/claude-skills/stargazers\"><img src=\"https://img.shields.io/github/stars/jeffallan/claude-skills?style=for-the-badge&color=yellow\" alt=\"Stars\"/></a>\n  <a href=\"https://github.com/jeffallan/claude-skills/actions/workflows/ci.yml\"><img src=\"https://img.shields.io/github/actions/workflow/status/jeffallan/claude-skills/ci.yml?branch=main&style=for-the-badge&label=CI\" alt=\"CI\"/></a>\n</p>\n\n<p align=\"center\">\n  <strong><!-- SKILL_COUNT -->66<!-- /SKILL_COUNT --> Skills</strong> | <strong><!-- WORKFLOW_COUNT -->9<!-- /WORKFLOW_COUNT --> Workflows</strong> | <strong>Context Engineering</strong> | <strong>Progressive Disclosure</strong>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/hesreallyhim/awesome-claude-code\"><img src=\"https://awesome.re/mentioned-badge.svg\" height=\"28\" alt=\"Mentioned in Awesome Claude Code\"/></a>\n  <a href=\"https://github.com/Chat2AnyLLM/awesome-claude-skills/blob/main/FULL-SKILLS.md\"><img src=\"https://img.shields.io/github/stars/Chat2AnyLLM/awesome-claude-skills?style=for-the-badge&label=awesome-claude-skills&color=brightgreen&logo=awesomelists&logoColor=white\" alt=\"Awesome Claude Skills\"/></a>\n  <a href=\"https://github.com/BehiSecc/awesome-claude-skills\"><img src=\"https://img.shields.io/github/stars/BehiSecc/awesome-claude-skills?style=for-the-badge&label=awesome-claude-skills&color=brightgreen&logo=awesomelists&logoColor=white\" alt=\"Awesome Claude Skills (BehiSecc)\"/></a>\n</p>\n\n---\n\n## Quick Start\n\n```bash\n/plugin marketplace add jeffallan/claude-skills\n```\nthen\n```bash\n/plugin install fullstack-dev-skills@jeffallan\n```\n\nFor all installation methods and first steps, see the [**Quick Start Guide**](QUICKSTART.md).\n\n**Full documentation:** [jeffallan.github.io/claude-skills](https://jeffallan.github.io/claude-skills)\n\n## Skills\n\n<!-- SKILL_COUNT -->66<!-- /SKILL_COUNT --> specialized skills across 12 categories covering languages, backend/frontend frameworks, infrastructure, APIs, testing, DevOps, security, data/ML, and platform specialists.\n\nSee [**Skills Guide**](SKILLS_GUIDE.md) for the full list, decision trees, and workflow combinations.\n\n## Usage Patterns\n\n### Context-Aware Activation\n\nSkills activate automatically based on your request:\n\n```bash\n# Backend Development\n\"Implement JWT authentication in my NestJS API\"\n‚Üí Activates: NestJS Expert ‚Üí Loads: references/authentication.md\n\n# Frontend Development\n\"Build a React component with Server Components\"\n‚Üí Activates: React Expert ‚Üí Loads: references/server-components.md\n```\n\n### Multi-Skill Workflows\n\nComplex tasks combine multiple skills:\n\n```\nFeature Development: Feature Forge ‚Üí Architecture Designer ‚Üí Fullstack Guardian ‚Üí Test Master ‚Üí DevOps Engineer\nBug Investigation:   Debugging Wizard ‚Üí Framework Expert ‚Üí Test Master ‚Üí Code Reviewer\nSecurity Hardening:  Secure Code Guardian ‚Üí Security Reviewer ‚Üí Test Master\n```\n\n## Context Engineering\n\nSurface and validate Claude's hidden assumptions about your project with `/common-ground`. See the [**Common Ground Guide**](docs/COMMON_GROUND.md) for full documentation.\n\n## Project Workflow\n\n<!-- WORKFLOW_COUNT -->9<!-- /WORKFLOW_COUNT --> workflow commands manage epics from discovery through retrospectives, integrating with Jira and Confluence. See <a href=\"docs/WORKFLOW_COMMANDS.md\"><strong>Workflow Commands Reference</strong></a> for the full command reference and lifecycle diagrams.\n&nbsp;\n\n> [!TIP]\n> **Setup:** Workflow commands require an Atlassian MCP server. See the [**Atlassian MCP Setup Guide**](docs/ATLASSIAN_MCP_SETUP.md).\n\n## Documentation\n\n- [**Quick Start Guide**](QUICKSTART.md) - Installation and first steps\n- [**Skills Guide**](SKILLS_GUIDE.md) - Skill reference and decision trees\n- [**Common Ground**](docs/COMMON_GROUND.md) - Context engineering with `/common-ground`\n- [**Workflow Commands**](docs/WORKFLOW_COMMANDS.md) - Project workflow commands guide\n- [**Atlassian MCP Setup**](docs/ATLASSIAN_MCP_SETUP.md) - Atlassian MCP server setup\n- [**Local Development**](docs/local_skill_development.md) - Local skill development\n- [**Contributing**](CONTRIBUTING.md) - Contribution guidelines\n- **skills/\\*/SKILL.md** - Individual skill documentation\n- **skills/\\*/references/** - Deep-dive reference materials\n\n## Contributing\n\nSee [**Contributing**](CONTRIBUTING.md) for guidelines on adding skills, writing references, and submitting pull requests.\n\n## Changelog\n\nSee [Changelog](CHANGELOG.md) for full version history and release notes.\n\n## License\n\nMIT License - See [LICENSE](LICENSE) file for details.\n\n## Support\n\n- **Issues:** [GitHub Issues](https://github.com/jeffallan/claude-skills/issues)\n- **Discussions:** [GitHub Discussions](https://github.com/jeffallan/claude-skills/discussions)\n- **Repository:** [github.com/jeffallan/claude-skills](https://github.com/jeffallan/claude-skills)\n\n## Author\n\nBuilt by [**jeffallan**](https://jeffallan.github.io) [<img src=\"https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linkedin/linkedin-original.svg\" width=\"16\" height=\"16\" alt=\"LinkedIn\"/>](https://www.linkedin.com/in/jeff-smolinski/)\n\n**Principal Consultant** at [**Synergetic Solutions**](https://synergetic.solutions) [<img src=\"https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linkedin/linkedin-original.svg\" width=\"16\" height=\"16\" alt=\"LinkedIn\"/>](https://www.linkedin.com/company/synergetic-holdings)\n\nFullstack engineering, security engineering, compliance, and technical due diligence.\n\n## Community\n\n[![Stargazers repo roster for @Jeffallan/claude-skills](https://reporoster.com/stars/Jeffallan/claude-skills)](https://github.com/Jeffallan/claude-skills/stargazers)\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Jeffallan/claude-skills&type=date&legend=top-left)](https://www.star-history.com/#Jeffallan/claude-skills&type=date&legend=top-left)\n\n---\n\n**Built for Claude Code** | **<!-- WORKFLOW_COUNT -->9<!-- /WORKFLOW_COUNT --> Workflows** | **<!-- REFERENCE_COUNT -->365<!-- /REFERENCE_COUNT --> Reference Files** | **<!-- SKILL_COUNT -->66<!-- /SKILL_COUNT --> Skills**\n"
  },
  {
    "full_name": "HandsOnLLM/Hands-On-Large-Language-Models",
    "name": "Hands-On-Large-Language-Models",
    "description": "Official code repo for the O'Reilly Book - \"Hands-On Large Language Models\"",
    "language": "Jupyter Notebook",
    "today_stars": "361",
    "total_stars": "21041",
    "metadata": {
      "topics": [
        "artificial-intelligence",
        "book",
        "large-language-models",
        "llm",
        "llms",
        "oreilly",
        "oreilly-books"
      ],
      "license": "Apache-2.0",
      "forks": 4983,
      "open_issues": 22,
      "created_at": "2024-06-28",
      "updated_at": "2026-02-13",
      "homepage": "https://www.llm-book.com/",
      "default_branch": "main",
      "size_kb": 13190,
      "watchers": 208,
      "archived": false
    },
    "readme_content": "Ôªø# Hands-On Large Language Models\r\n\r\n<a href=\"https://www.linkedin.com/in/jalammar/\"><img src=\"https://img.shields.io/badge/Follow%20Jay-blue.svg?logo=linkedin\"></a>\r\n<a href=\"https://www.linkedin.com/in/mgrootendorst/\"><img src=\"https://img.shields.io/badge/Follow%20Maarten-blue.svg?logo=linkedin\"></a>\r\n<a href=\"https://www.deeplearning.ai/short-courses/how-transformer-llms-work/?utm_campaign=handsonllm-launch&utm_medium=partner\"><img src=\"https://img.shields.io/badge/DeepLearning.AI%20Course-NEW!-&labelColor=black&color=red.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAuMDAwMzY1MjgxIC0wLjAwMDE0MDE0MiAzMy4yOSAzMy4xNSI+Cgk8cGF0aCBkPSJNMTYuNjQzIDMzLjE0NWMtMy4yOTIgMC02LjUxLS45NzItOS4yNDYtMi43OTNhMTYuNTg4IDE2LjU4OCAwIDAxLTYuMTMtNy40MzhBMTYuNTA3IDE2LjUwNyAwIDAxLjMyIDEzLjM0YTE2LjU1IDE2LjU1IDAgMDE0LjU1NS04LjQ4NUExNi42NjUgMTYuNjY1IDAgMDExMy4zOTYuMzE4YTE2LjcxIDE2LjcxIDAgMDE5LjYxNi45NDQgMTYuNjI4IDE2LjYyOCAwIDAxNy40NyA2LjEwMyAxNi41MjIgMTYuNTIyIDAgMDEyLjgwNCA5LjIwN2MwIDQuMzk2LTEuNzUzIDguNjEtNC44NzQgMTEuNzE5YTE2LjY4IDE2LjY4IDAgMDEtMTEuNzY5IDQuODU0em0uMTI1LTYuNjI4YzYuOTA2IDAgMTIuNTE3LTUuNjk4IDEyLjUxNy0xMi43MyAwLTcuMDMtNS42MS0xMi43MjUtMTIuNTE3LTEyLjcyNS02LjkwNiAwLTEyLjUxNyA1LjY5OC0xMi41MTcgMTIuNzI1IDAgNy4wMjcgNS42MTEgMTIuNzMgMTIuNTE3IDEyLjczem0tLjEyNS0yLjkxOGMtNi4yODkgMC0xMS4zODYtNC45MjUtMTEuMzg2LTExLjAwMkM1LjI1NyA2LjUyIDEwLjM2IDEuNTkgMTYuNjQzIDEuNTljNi4yODQgMCAxMS4zODYgNC45MyAxMS4zODYgMTEuMDA3cy01LjA5NyAxMS4wMDItMTEuMzg2IDExLjAwMnptLS4yNDItNC41MDhjNC43NyAwIDguNjMzLTMuNjc5IDguNjMzLTguMjE4IDAtNC41MzgtMy44ODUtOC4yMjEtOC42MzMtOC4yMjEtNC43NDcgMC04LjYzMiAzLjY3OS04LjYzMiA4LjIyMSAwIDQuNTQzIDMuODg1IDguMjE4IDguNjMyIDguMjE4eiIgZmlsbD0iI0ZENEE2MSIvPgo8L3N2Zz4=\"></a>\r\n\r\nWelcome! In this repository you will find the code for all examples throughout the book [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) written by [Jay Alammar](https://www.linkedin.com/in/jalammar/) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/) which we playfully dubbed: <br> \r\n\r\n<p align=\"center\"><b><i>\"The Illustrated LLM Book\"</i></b></p>\r\n\r\nThrough the visually educational nature of this book and with **almost 300 custom made figures**, learn the practical tools and concepts you need to use Large Language Models today!\r\n\r\n<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"><img src=\"images/book_cover.png\" width=\"50%\" ></a>\r\n\r\n<br>\r\n\r\nThe book is available on:\r\n\r\n* [Amazon](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961)\r\n* [Shroff Publishers (India)](https://www.shroffpublishers.com/books/computer-science/large-language-models/9789355425522/)\r\n* [O'Reilly](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/)\r\n* [Kindle](https://www.amazon.com/Hands-Large-Language-Models-Alammar-ebook/dp/B0DGZ46G88/ref=tmm_kin_swatch_0?_encoding=UTF8&qid=&sr=)\r\n* [Barnes and Noble](https://www.barnesandnoble.com/w/hands-on-large-language-models-jay-alammar/1145185960)\r\n* [Goodreads](https://www.goodreads.com/book/show/210408850-hands-on-large-language-models)\r\n\r\n## Table of Contents\r\n\r\nWe advise to run all examples through Google Colab for the easiest setup. Google Colab allows you to use a T4 GPU with 16GB of VRAM for free. All examples were mainly built and tested using Google Colab, so it should be the most stable platform. However, any other cloud provider should work. \r\n\r\n| Chapter  | Notebook  |\r\n|---|---|\r\n| Chapter 1: Introduction to Language Models  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter01/Chapter%201%20-%20Introduction%20to%20Language%20Models.ipynb)   |\r\n| Chapter 2: Tokens and Embeddings  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter02/Chapter%202%20-%20Tokens%20and%20Token%20Embeddings.ipynb)  |\r\n| Chapter 3: Looking Inside Transformer LLMs  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter03/Chapter%203%20-%20Looking%20Inside%20LLMs.ipynb)  |\r\n| Chapter 4: Text Classification  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter04/Chapter%204%20-%20Text%20Classification.ipynb)  |\r\n| Chapter 5: Text Clustering and Topic Modeling  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter05/Chapter%205%20-%20Text%20Clustering%20and%20Topic%20Modeling.ipynb)  |\r\n| Chapter 6: Prompt Engineering  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter06/Chapter%206%20-%20Prompt%20Engineering.ipynb)  |\r\n| Chapter 7: Advanced Text Generation Techniques and Tools  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter07/Chapter%207%20-%20Advanced%20Text%20Generation%20Techniques%20and%20Tools.ipynb)  |\r\n| Chapter 8: Semantic Search and Retrieval-Augmented Generation  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter08/Chapter%208%20-%20Semantic%20Search.ipynb)  |\r\n| Chapter 9: Multimodal Large Language Models  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter09/Chapter%209%20-%20Multimodal%20Large%20Language%20Models.ipynb)  |\r\n| Chapter 10: Creating Text Embedding Models  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter10/Chapter%2010%20-%20Creating%20Text%20Embedding%20Models.ipynb)  |\r\n| Chapter 11: Fine-tuning Representation Models for Classification  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter11/Chapter%2011%20-%20Fine-Tuning%20BERT.ipynb)  |\r\n| Chapter 12: Fine-tuning Generation Models  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter12/Chapter%2012%20-%20Fine-tuning%20Generation%20Models.ipynb)  |\r\n\r\n> [!TIP]\r\n> You can check the [setup](.setup/) folder for a quick-start guide to install all packages locally and you can check the [conda](.setup/conda/) folder for a complete guide on how to setup your environment, including conda and PyTorch installation.\r\n> Note that the depending on your OS, Python version, and dependencies your results might be slightly differ. However, they\r\n> should this be similar to the examples in the book. \r\n\r\n\r\n## Reviews\r\n\r\n> \"*Jay and Maarten have continued their tradition of providing beautifully illustrated and insightful descriptions of complex topics in their new book. Bolstered with working code, timelines, and references to key papers, their book is a valuable resource for anyone looking to understand the main techniques behind how Large Language Models are built.*\"\r\n>    \r\n> **Andrew Ng** - founder of [DeepLearning.AI](https://www.deeplearning.ai/)\r\n\r\n---\r\n\r\n> \"*This is an exceptional guide to the world of language models and their practical applications in industry. Its highly-visual coverage of generative, representational, and retrieval applications of language models empowers readers to quickly understand, use, and refine LLMs. Highly recommended!*\"\r\n>\r\n> **Nils Reimers** - Director of Machine Learning at Cohere | creator of [sentence-transformers](https://github.com/UKPLab/sentence-transformers)\r\n\r\n---\r\n\r\n> \"*I can‚Äôt think of another book that is more important to read right now. On every single page, I learned something that is critical to success in this era of language models.*\"\r\n> \r\n> **Josh Starmer** - [StatQuest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)\r\n\r\n---\r\n\r\n> \"*If you‚Äôre looking to get up to speed in everything regarding LLMs, look no further! In this wonderful book, Jay and Maarten will take you from zero to expert in the history and latest advances in large language models. With very intuitive explanations, great real-life examples, clear illustrations, and comprehensive code labs, this book lifts the curtain on the complexities of transformer models, tokenizers, semantic search, RAG, and many other cutting-edge technologies. A must read for anyone interested in the latest AI technology!*\"\r\n> \r\n> **Luis Serrano, PhD** - Founder and CEO of [Serrano Academy](https://www.youtube.com/@SerranoAcademy)\r\n\r\n---\r\n\r\n> \"*Hands-On Large Language Models brings clarity and practical examples to cut through the hype of AI. It provides a wealth of great diagrams and visual aids to supplement the clear explanations. The worked examples and code make concrete what other books leave abstract. The book starts with simple introductory beginnings, and steadily builds in scope. By the final chapters, you will be fine-tuning and building your own large language models with confidence.*\"\r\n>\r\n> **Leland McInnes** - Researcher at the Tutte Institute for Mathematics and Computing | creator of [UMAP](https://github.com/lmcinnes/umap) and [HDBSCAN](https://github"
  }
]